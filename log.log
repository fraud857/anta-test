2026-01-14 20:05:23 INFO  Worker: 10.131.9.72:7800 receives the metadata collection task2026-01-14 20:05:23 INFO  Metadata collection parameters information:2026-01-14 20:05:23 INFO  【taskId】:1913, 【recordId】:636752026-01-14 20:05:23 INFO  【datasource type】:HBASE2026-01-14 20:05:23 INFO  【datasource id】:20099038890101473302026-01-14 20:05:23 INFO  【datasource connection id】:20099038892701941782026-01-14 20:05:23 INFO  【datasource jdbc url】:null2026-01-14 20:05:23 INFO  【database name】:SHUXIN_TEST22026-01-14 20:05:23 INFO  【schema name】:SHUXIN_TEST22026-01-14 20:05:23 INFO  【use DBA Table】:false2026-01-14 20:05:23 ERROR An exception occurs in the collection task com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: 执行异常	at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:82)	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(&lt;generated&gt;)	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)	at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)	at java.base/java.lang.Thread.run(Thread.java:829)2026-01-14 20:05:23 INFO  The collection task has been completed, A total of 0 tables have been collected.2026-01-14 20:05:23 INFO  Clean up the collection context information2026-01-14 20:05:23 INFO  END-EOF












2026-01-13 16:23:21,336 | INFO  | [pool-39-thread-1] | Trying to connect to metastore with URI thrift://MRS-ADP-Q02-node-master3QKOz.mrs-h69n.com:9083 | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:777)
2026-01-13 16:23:21,337 | INFO  | [pool-39-thread-1] | HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection. | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:844)
2026-01-13 16:23:21,337 | INFO  | [pool-39-thread-1] | Whether to use hadoop rpc protection: true | org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.getMetaStoreSaslProperties(MetaStoreUtils.java:1147)
2026-01-13 16:23:21,346 | INFO  | [pool-39-thread-1] | Opened a connection to metastore, current connections: 3 | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:871)
2026-01-13 16:23:21,347 | INFO  | [pool-39-thread-1] | Connected to metastore. | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:967)
2026-01-13 16:23:21,347 | INFO  | [pool-39-thread-1] | RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=p_SuperAdmin@950C233D_AB6E_4AD9_AB28_F2766F074340.COM (auth:KERBEROS) retries=1 delay=1 lifetime=0 | org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:99)
2026-01-13 16:23:21,881 | WARN  | [pool-39-thread-1] | Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf | org.apache.hudi.common.config.DFSPropertiesConfiguration.getConfPathFromEnv(DFSPropertiesConfiguration.java:235)
2026-01-13 16:23:21,912 | WARN  | [pool-39-thread-1] | Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file | org.apache.hudi.common.config.DFSPropertiesConfiguration.addPropsFromFile(DFSPropertiesConfiguration.java:158)
2026-01-13 16:23:23,572 | INFO  | [pool-39-thread-1] | It took 1108 ms to build plan. | org.apache.spark.sql.execution.QueryExecution.logInfo(Logging.scala:60)
2026-01-13 16:23:24,290 | INFO  | [pool-39-thread-1] | Code generated in 429.98129 ms | org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.logInfo(Logging.scala:60)
2026-01-13 16:23:24,492 | INFO  | [pool-39-thread-1] | Block broadcast_0 stored as values in memory (estimated size 777.7 KiB, free 433.6 MiB) | org.apache.spark.storage.memory.MemoryStore.logInfo(Logging.scala:60)
2026-01-13 16:23:24,961 | INFO  | [pool-39-thread-1] | Block broadcast_0_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 433.6 MiB) | org.apache.spark.storage.memory.MemoryStore.logInfo(Logging.scala:60)
2026-01-13 16:23:24,965 | INFO  | [dispatcher-BlockManagerMaster] | Added broadcast_0_piece0 in memory on MRS-ADP-Q02-hdfs-groupMUwL0001.mrs-h69n.com:22787 (size: 91.4 KiB, free: 434.3 MiB) | org.apache.spark.storage.BlockManagerInfo.logInfo(Logging.scala:60)
2026-01-13 16:23:24,998 | INFO  | [pool-39-thread-1] | Created broadcast 0 from  | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:25,255 | WARN  | [pool-39-thread-1] | Request failed, Response code: 403; Request ID: 0000019BB673DD6981099C0EF15CDFC0; Request path: http://adp-cluster-dev.obs.cn-east-273.antacloud.com/data%2Fhive%2Ftest01.db%2Fb_foitem_copy | com.obs.services.internal.RestStorageService.invoke0(NativeMethodAccessorImpl.java:-2)
2026-01-13 16:23:25,257 | WARN  | [pool-39-thread-1] | Storage|1|HTTP+XML|getObjectMetadata||||2026-01-13 16:23:25|2026-01-13 16:23:25|||403| | com.obs.services.AbstractClient.invoke0(NativeMethodAccessorImpl.java:-2)
2026-01-13 16:23:25,266 | ERROR | [pool-39-thread-1] | sql execute error:SELECT * from test01.b_foitem_copy | com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:228)
2026-01-13 16:23:29,996 | ERROR | [Driver] | org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0] | com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:244)
java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_462]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242) [custom-spark-submit-project.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_462]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_462]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_462]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_462]
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740) [spark-yarn_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.exception.ObsException: com.obs.services.exception.ObsException: Error message:Request Error.OBS service Error Message. -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAAheaders{error-message:Access Denied,request-id:0000019BB673DD6981099C0EF15CDFC0,content-length:0,id-2:36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA,error-code:AccessDenied,x-reserved-indicator:396,date:Tue, 13 Jan 2026 08:23:25 GMT,}
	at com.obs.services.internal.utils.ServiceUtils.changeFromServiceException(ServiceUtils.java:579) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:408) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.internal.ServiceException: Request Error.
	at com.obs.services.internal.RestStorageService.createServiceException(RestStorageService.java:702) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.handleRequestErrorResponse(RestStorageService.java:526) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.tryRequest(RestStorageService.java:512) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRequest(RestStorageService.java:387) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRestHead(RestStorageService.java:980) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectImpl(ObsObjectBaseService.java:229) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectMetadataImpl(ObsObjectBaseService.java:401) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.access$2000(AbstractObjectClient.java:60) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:665) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:661) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:398) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
2026-01-13 16:23:30,000 | INFO  | [Driver] | SparkContext is stopping with exitCode 0. | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:30,030 | INFO  | [Driver] | Stopped Spark web UI at http://MRS-ADP-Q02-hdfs-groupMUwL0001.mrs-h69n.com:22803 | org.apache.spark.ui.SparkUI.logInfo(Logging.scala:60)
2026-01-13 16:23:30,060 | INFO  | [Driver] | Shutting down all executors | org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend.logInfo(Logging.scala:60)
2026-01-13 16:23:30,064 | INFO  | [dispatcher-CoarseGrainedScheduler] | Asking each executor to shut down | org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint.logInfo(Logging.scala:60)
2026-01-13 16:23:30,200 | INFO  | [dispatcher-event-loop-0] | MapOutputTrackerMasterEndpoint stopped! | org.apache.spark.MapOutputTrackerMasterEndpoint.logInfo(Logging.scala:60)
2026-01-13 16:23:30,223 | INFO  | [Driver] | MemoryStore cleared | org.apache.spark.storage.memory.MemoryStore.logInfo(Logging.scala:60)
2026-01-13 16:23:30,224 | INFO  | [Driver] | BlockManager stopped | org.apache.spark.storage.BlockManager.logInfo(Logging.scala:60)
2026-01-13 16:23:30,246 | INFO  | [Driver] | BlockManagerMaster stopped | org.apache.spark.storage.BlockManagerMaster.logInfo(Logging.scala:60)
2026-01-13 16:23:30,252 | INFO  | [dispatcher-event-loop-1] | OutputCommitCoordinator stopped! | org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint.logInfo(Logging.scala:60)
2026-01-13 16:23:30,273 | INFO  | [Driver] | Successfully stopped SparkContext | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:30,273 | INFO  | [Driver] | SparkContext is stopping with exitCode 0. | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:30,273 | INFO  | [Driver] | SparkContext already stopped. | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
All resources have been released.
2026-01-13 16:23:30,275 | ERROR | [Driver] | User class threw exception:  | org.apache.spark.deploy.yarn.ApplicationMaster.logError(Logging.scala:97)
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:248) ~[custom-spark-submit-project.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_462]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_462]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_462]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_462]
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740) [spark-yarn_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_462]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242) ~[custom-spark-submit-project.jar:?]
	... 5 more
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.exception.ObsException: com.obs.services.exception.ObsException: Error message:Request Error.OBS service Error Message. -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAAheaders{error-message:Access Denied,request-id:0000019BB673DD6981099C0EF15CDFC0,content-length:0,id-2:36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA,error-code:AccessDenied,x-reserved-indicator:396,date:Tue, 13 Jan 2026 08:23:25 GMT,}
	at com.obs.services.internal.utils.ServiceUtils.changeFromServiceException(ServiceUtils.java:579) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:408) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.internal.ServiceException: Request Error.
	at com.obs.services.internal.RestStorageService.createServiceException(RestStorageService.java:702) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.handleRequestErrorResponse(RestStorageService.java:526) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.tryRequest(RestStorageService.java:512) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRequest(RestStorageService.java:387) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRestHead(RestStorageService.java:980) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectImpl(ObsObjectBaseService.java:229) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectMetadataImpl(ObsObjectBaseService.java:401) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.access$2000(AbstractObjectClient.java:60) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:665) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:661) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:398) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
2026-01-13 16:23:30,293 | INFO  | [Driver] | Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740)
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242)
	... 5 more
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691)
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564)
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527)
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936)
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942)
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145)
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.obs.services.exception.ObsException: Error message:Request Error.OBS service Error Message. -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAAheaders{error-message:Access Denied,request-id:0000019BB673DD6981099C0EF15CDFC0,content-length:0,id-2:36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA,error-code:AccessDenied,x-reserved-indicator:396,date:Tue, 13 Jan 2026 08:23:25 GMT,}
	at com.obs.services.internal.utils.ServiceUtils.changeFromServiceException(ServiceUtils.java:579)
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:408)
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660)
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164)
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546)
	... 64 more
Caused by: com.obs.services.internal.ServiceException: Request Error. HEAD 'http://adp-cluster-dev.obs.cn-east-273.antacloud.com/data%2Fhive%2Ftest01.db%2Fb_foitem_copy' on Host 'adp-cluster-dev.obs.cn-east-273.antacloud.com' @ 'Tue, 13 Jan 2026 08:23:25 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA
	at com.obs.services.internal.RestStorageService.createServiceException(RestStorageService.java:702)
	at com.obs.services.internal.RestStorageService.handleRequestErrorResponse(RestStorageService.java:526)
	at com.obs.services.internal.RestStorageService.tryRequest(RestStorageService.java:512)
	at com.obs.services.internal.RestStorageService.performRequest(RestStorageService.java:387)
	at com.obs.services.internal.RestStorageService.performRestHead(RestStorageService.java:980)
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectImpl(ObsObjectBaseService.java:229)
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectMetadataImpl(ObsObjectBaseService.java:401)
	at com.obs.services.AbstractObjectClient.access$2000(AbstractObjectClient.java:60)
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:665)
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:661)
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:398)
	... 67 more
) | org.apache.spark.deploy.yarn.ApplicationMaster.logInfo(Logging.scala:60)
2026-01-13 16:23:30,313 | INFO  | [shutdown-hook-0] | Deleting staging directory hdfs://hacluster/user/p_SuperAdmin/.sparkStaging/application_1768094372387_0569 | org.apache.spark.deploy.yarn.ApplicationMaster.logInfo(Logging.scala:60)
2026-01-13 16:23:30,334 | INFO  | [shutdown-hook-0] | Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740)
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242)
	... 5 more
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691)
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564)
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527)
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936)
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942)
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145)
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)



















2026-01-13 13:42:59 INFO  Current task status: Running2026-01-13 13:42:59 INFO  Start execute SPARK_SQL on worker_host: 10.131.11.232:78002026-01-13 13:42:59 INFO  Current log dir: /opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/1768282979365.log2026-01-13 13:42:59 INFO  Task executor: com.datacyber.cyberdata.worker.service.SparkSqlExecutorService2026-01-13 13:42:59 INFO  Start executing the Spark failover process2026-01-13 13:42:59 INFO  The spark applicationId is empty, preparing to start a new spark application2026-01-13 13:42:59 INFO  The environment is initialized2026-01-13 13:42:59 INFO  prepare the spark submit script......2026-01-13 13:42:59 INFO  spark task submit on yarn with depoly-mode cluster: 1001 2 iam_spark_dev2026-01-13 13:42:59 INFO  hadoop default fs: hdfs://hacluster2026-01-13 13:42:59 INFO  sql: SELECT * from test01.b_foitem_copy;2026-01-13 13:42:59 INFO  spark command:/opt/module/spark-3.5.6-mrs/bin/spark-submit --class com.datacyber.cyberdata.spark.CustomSqlHandler --master yarn --deploy-mode cluster --name test_saprk --conf 'spark.executor.memory=2g' --conf 'spark.driver.memory=1g' --conf 'spark.driver.cores=1' --conf 'spark.executor.cores=1' --conf 'spark.executor.instances=1' --conf 'spark.yarn.queue=default' /opt/module/spark-3.5.6-mrs/jars/custom-spark-submit-project.jar U0VMRUNUICogZnJvbSB0ZXN0MDEuYl9mb2l0ZW1fY29weTs= http://10.131.0.85:30303/worker/writeLog /opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/results true  2026-01-13 13:42:59 INFO  spark-submit command : /bin/bash /home/datac/shell/2010950683592646657.sh2026-01-13 13:42:59 INFO  Successful prepare the spark submit script......2026-01-13 13:42:59 WARN  1001,2,iam_spark_dev, kerberos info: null2026-01-13 13:42:59 INFO  Temporary ENV Variables2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  {}2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  Full Command2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  /bin/bash /home/datac/shell/2010950683592646657.sh2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  --- Invoking Shell command line now ---2026-01-13 13:42:59 INFO  =================================================================Warning: Ignoring non-Spark config property: hoodie.schema.evolution.enableWarning: Ignoring non-Spark config property: hoodie.archive.delete.parallelismWarning: Ignoring non-Spark config property: hoodie.clean.asyncWarning: Ignoring non-Spark config property: hoodie.clean.trigger.strategyWarning: Ignoring non-Spark config property: hoodie.use.hive.write.styleWarning: Ignoring non-Spark config property: hoodie.keep.min.commitsWarning: Ignoring non-Spark config property: token.server.fs.obs.endpointWarning: Ignoring non-Spark config property: hoodie.datasource.write.keygenerator.classWarning: Ignoring non-Spark config property: hoodie.datasource.write.payload.classWarning: Ignoring non-Spark config property: hoodie.cleaner.policyWarning: Ignoring non-Spark config property: fs.obs.delegation.token.onlyWarning: Ignoring non-Spark config property: hoodie.cleaner.parallelismWarning: Ignoring non-Spark config property: hoodie.parquet.compression.codecWarning: Ignoring non-Spark config property: hoodie.datasource.hive_sync.enableWarning: Ignoring non-Spark config property: hoodie.bulkinsert.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.parquet.small.file.limitWarning: Ignoring non-Spark config property: hoodie.cleaner.commits.retainedWarning: Ignoring non-Spark config property: hoodie.compact.inline.max.delta.commitsWarning: Ignoring non-Spark config property: hoodie.delete.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.archive.hours.retainedWarning: Ignoring non-Spark config property: hoodie.archive.policyWarning: Ignoring non-Spark config property: hoodie.upsert.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.clean.automaticWarning: Ignoring non-Spark config property: fs.obs.delegation.token.providers.ccWarning: Ignoring non-Spark config property: hoodie.compact.inlineWarning: Ignoring non-Spark config property: hoodie.insert.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.datasource.write.hive_style_partitioningWarning: Ignoring non-Spark config property: hoodie.datasource.hive_sync.modeWarning: Ignoring non-Spark config property: hoodie.archive.asyncWarning: Ignoring non-Spark config property: hoodie.upgrade.enableWarning: Ignoring non-Spark config property: hoodie.cleaner.hours.retainedWarning: Ignoring non-Spark config property: hoodie.datasource.hive_sync.support_timestampWarning: Ignoring non-Spark config property: hoodie.rollback.parallelismWarning: Ignoring non-Spark config property: hoodie.archive.automaticWarning: Ignoring non-Spark config property: hoodie.schedule.compact.only.inlineWarning: Ignoring non-Spark config property: hoodie.keep.max.commitsWarning: Ignoring non-Spark config property: hoodie.run.compact.only.inline26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.rpc.numRetries' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.yarn.kerberos.relogin.period' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.relogin.period' instead.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.executor.plugins' has been deprecated as of Spark 3.0.0 and may be removed in the future. Feature replaced with new plugin API. See Monitoring documentation.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.reducer.maxReqSizeShuffleToMem' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.rpc.retry.wait' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.rpc.numRetries' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.yarn.kerberos.relogin.period' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.relogin.period' instead.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.executor.plugins' has been deprecated as of Spark 3.0.0 and may be removed in the future. Feature replaced with new plugin API. See Monitoring documentation.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.reducer.maxReqSizeShuffleToMem' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.rpc.retry.wait' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:18 INFO AbstractService: Service org.apache.hadoop.yarn.client.api.impl.YarnClientImpl failed in state STARTEDjava.lang.IllegalArgumentException: Can't get Kerberos realm	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:78)	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:322)	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:307)	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:582)	at org.apache.hadoop.yarn.client.RMProxy.&lt;init&gt;(RMProxy.java:72)	at org.apache.hadoop.yarn.client.ClientRMProxy.&lt;init&gt;(ClientRMProxy.java:66)	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:79)	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:261)	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:202)	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:213)	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1403)	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1871)	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1076)	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:201)	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:224)	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:93)	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1167)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1176)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalArgumentException: KrbException: Cannot locate default realm	at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.&lt;init&gt;(KerberosPrincipal.java:179)	at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:128)	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:76)	... 18 moreException in thread "main" java.lang.IllegalArgumentException: Can't get Kerberos realm	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:78)	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:322)	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:307)	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:582)	at org.apache.hadoop.yarn.client.RMProxy.&lt;init&gt;(RMProxy.java:72)	at org.apache.hadoop.yarn.client.ClientRMProxy.&lt;init&gt;(ClientRMProxy.java:66)	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:79)	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:261)	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:202)	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:213)	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1403)	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1871)	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1076)	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:201)	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:224)	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:93)	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1167)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1176)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalArgumentException: KrbException: Cannot locate default realm	at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.&lt;init&gt;(KerberosPrincipal.java:179)	at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:128)	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:76)	... 18 more26/01/13 13:44:18 INFO ShutdownHookManager: Shutdown hook called26/01/13 13:44:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-8cdb8aa8-b6dc-49f0-b17a-d365ca6a58d6


2026-01-13 13:42:59.482 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - -----read
2026-01-13 13:42:59.482 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - accessLogs size:0
2026-01-13 13:42:59.482 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - getParseInfoByMqDtoOver:[]
2026-01-13 13:42:59.483 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - 1 spark: taskDispatchMqDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010950683592646657, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 13:42:59 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/1768282979365.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 13:42:59.483 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: 执行executeTask线程名称:SPARK_SQL-TaskExecutor-2010950683592646657
2026-01-13 13:42:59.483 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - 2 spark: currentDispatchDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010950683592646657, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 13:42:59 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/1768282979365.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 13:42:59.483 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover获取到的appId为空
2026-01-13 13:42:59.483 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover结果:false
2026-01-13 13:42:59.486 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.common.utils.TaskVariablesUtils - functionValueMap2:{}
2026-01-13 13:42:59.486 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.common.utils.TaskVariablesUtils - replaceContent:SELECT * from test01.b_foitem_copy;
2026-01-13 13:42:59.495 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - 未找到引擎集群: clusterName=iam_spark_dev, tenantId=1001
2026-01-13 13:42:59.497 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkSqlExecutorService - federated datasource info: []
2026-01-13 13:42:59.497 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkSqlExecutorService - federated query status:false
2026-01-13 13:42:59.499 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - 未找到引擎集群: clusterName=iam_spark_dev, tenantId=1001
2026-01-13 13:42:59.521 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.utils.ProcessUtil - processClass name:java.lang.ProcessImpl
2026-01-13 13:43:00.000 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.scheduler.WorkerSelfRegistry - HealthCheckEnable=true
2026-01-13 13:43:00.023 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.api.service.impl.WorkerRpcServiceImpl - Heartbeat report url:http://coordinator-inner-dev:7500/coordinator/worker/heartbeat, request={"active":true,"availableThread":62,"cpuUsage":0.21,"createTime":"2026-01-13T11:17:37.051910000","env":"all","host":"10.131.11.232","jvmFreeMemory":2208031312,"jvmMaxMemory":6442450944,"maxCpu":3.0,"maxThread":64,"port":7800,"reservedJvmMb":7168.0,"resourceGroupNameEn":"default","sysFreeMemory":7454822400,"sysMaxMemory":12884901888,"type":"worker","updateTime":"2026-01-13T13:43:00.006677000","usedCpu":1.0,"usedJvmMb":2.0,"usedMemMb":1000.0}, response={"code":"200","data":true,"msg":"成功"}
2026-01-13 13:43:01.496 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-13 13:43:04.534 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描..






2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - start parseTableIdInfo,tableId:TableId(catalogName=null, schemaName=test01, tableName=b_foitem_copy, metaType=null, tableALias=null),datasourceInfo:mrs_hive_spark_test,currentDB:,currentSchema:default, tempTableMapInfo:{}
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - tableName:b_foitem_copy
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - current schema:test01
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - start get mrs_hive_spark_test 's datasourceName,db:test01
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - data connections size:4
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - current db's datasource name:mrs_hive_spark_test,env:2
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - current datasource name:(mrs_hive_spark_test,2),currentEnv:2
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - start checkIsOwner,catalog:mrs_hive_spark_test360,database:null,schema:test01,tableName:b_foitem_copy
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - ownerId:1
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - performUserId:1 is owner
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - -----read
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - accessLogs size:0
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - getParseInfoByMqDtoOver:[]
2026-01-13 09:36:47.107 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - 1 spark: taskDispatchMqDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010888723605905410, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 09:36:46 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/1768268206981.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: 执行executeTask线程名称:SPARK_SQL-TaskExecutor-2010888723605905410
2026-01-13 09:36:47.107 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - 2 spark: currentDispatchDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010888723605905410, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 09:36:46 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/1768268206981.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover获取到的appId为空
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover结果:false
2026-01-13 09:36:47.110 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager - 集群:1001~~2~~iam_spark_dev 加载失败: java.lang.IllegalArgumentException: kerberosConf can not null, engine code: MRS_360_SPARK_TEST_DEV
        at org.springframework.util.Assert.notNull(Assert.java:201)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.downloadClusterConfig(ClusterEngineManager.java:596)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.loadEngine(ClusterEngineManager.java:223)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.lambda$afterPropertiesSet$0(ClusterEngineManager.java:141)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

2026-01-13 09:36:47.110 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - Task execute failed 
com.datacyber.cyberdata.worker.exception.WorkerException: 集群:1001~~2~~iam_spark_dev 加载失败: java.lang.IllegalArgumentException: kerberosConf can not null, engine code: MRS_360_SPARK_TEST_DEV
        at org.springframework.util.Assert.notNull(Assert.java:201)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.downloadClusterConfig(ClusterEngineManager.java:596)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.loadEngine(ClusterEngineManager.java:223)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.lambda$afterPropertiesSet$0(ClusterEngineManager.java:141)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.getHadoopConfigDir(ClusterEngineManager.java:463)
        at com.datacyber.cyberdata.worker.service.SparkSqlExecutorService.buildYarnCommandOfNew(SparkSqlExecutorService.java:115)
        at com.datacyber.cyberdata.worker.service.SparkSqlExecutorService.buildCommand(SparkSqlExecutorService.java:94)
        at com.datacyber.cyberdata.worker.service.SparkExecutorService.executeTask(SparkExecutorService.java:178)
        at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor.runTask(AbstractTaskExecutor.java:310)
        at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor$$FastClassBySpringCGLIB$$4b20b1cc.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-13 09:36:47.111 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark application id is empty
2026-01-13 09:36:47.111 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - handleInstanceState: localVariables:{}












日志获取中
2026-01-12 09:59:11 INFO  Worker: 10.131.8.25:7800 receives the metadata collection task
2026-01-12 09:59:11 INFO  Metadata collection parameters information:
2026-01-12 09:59:11 INFO  【taskId】:1911, 【recordId】:63391
2026-01-12 09:59:11 INFO  【datasource type】:HIVE
2026-01-12 09:59:11 INFO  【datasource id】:2009875419122475009
2026-01-12 09:59:11 INFO  【datasource connection id】:2009875419193778177
2026-01-12 09:59:11 INFO  【datasource jdbc url】:jdbc:hive2://mrs-adp-q02-node-master2femp.mrs-h69n.com:22550/test01_dev
2026-01-12 09:59:11 INFO  【database name】:test01_dev
2026-01-12 09:59:58 ERROR Hive getMetaData occurred exception.
java.lang.RuntimeException: Authentication: simple 操作失败: null
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.runSecured(Hive3Plugin.java:103)
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.getConnection(Hive3Plugin.java:198)
	at com.datacyber.cyberdata.worker.service.accessor.impl.HiveAccessService.getMetaData(HiveAccessService.java:130)
	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.lambda$collect$0(MetadataCollectorService.java:253)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.reflect.UndeclaredThrowableException: null
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1894)
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.runSecured(Hive3Plugin.java:99)
	... 8 common frames omitted
Caused by: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://mrs-adp-q02-node-master2femp.mrs-h69n.com:22550/test01_dev: Peer indicated failure: PLAIN auth failed: org.apache.hive.service.auth.basic.BasicAuthenticationException: user[anonymous] basic authentication failed.
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:256)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
	at com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactoryNoPool.openConnection(DriverConnectionFactoryNoPool.java:50)
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.getHiveConnection(Hive3Plugin.java:203)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	... 9 common frames omitted
Caused by: org.apache.thrift.transport.TTransportException: Peer indicated failure: PLAIN auth failed: org.apache.hive.service.auth.basic.BasicAuthenticationException: user[anonymous] basic authentication failed.
	at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:199)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:307)
	at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
	at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:343)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:228)
	... 17 common frames omitted
2026-01-12 09:59:58 INFO  Thread【MetaDataCollector_sub_thread_63391_0】Complete the current batch collection, table of count: 0
2026-01-12 09:59:58 INFO  Start collecting object information.
2026-01-12 09:59:58 INFO  Complete collect object information.
2026-01-12 09:59:58 INFO  The metadata is retrieved and ready to be written to persistent storage.
2026-01-12 09:59:58 INFO  A total of 0 tables are found in this collection task, which are written to persistent storage in 0 batches.
2026-01-12 09:59:58 INFO  Writes to persistent storage have been completed
2026-01-12 09:59:58 INFO  The collection task has been completed, A total of 0 tables have been collected.
2026-01-12 09:59:58 INFO  Clean up the collection context information
2026-01-12 09:59:58 INFO  END-EOF

帮助




25/11/27 17:34:04 INFO RetryInvocationHandler: Call From cyber-worker-test-8574cd4775-m8xkp/10.131.15.83 to mrs-adp-q02-node-master2femp.mrs-h69n.com:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over 18 after 19 failover attempts. Trying to failover after sleeping for 42745ms.
25/11/27 17:34:47 INFO ConfiguredRMFailoverProxyProvider: Failing over to 19
25/11/27 17:34:47 INFO RetryInvocationHandler: DestHost:destPort mrs-adp-q02-node-master3qkoz.mrs-h69n.com:8032 , LocalHost:localPort cyber-worker-test-8574cd4775-m8xkp/10.131.15.83:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Server has invalid Kerberos principal: mapred/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@B2D05EE0_E921_40A8_A6FD_DCB7693B106B.COM, expecting: mapred/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM, while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over 19 after 20 failover attempts. Trying to failover after sleeping for 33880ms.



2025-11-17 14:09:38.002 ERROR [cyber-platform] [http-nio-7600-exec-113] [] com.datacyber.cyberdata.service.core.exception.CoreExceptionHandler - Exception: request: /standard/field/export/template raised List validation with explicit values must specify at least one value
java.lang.IllegalArgumentException: List validation with explicit values must specify at least one value
        at org.apache.poi.xssf.usermodel.XSSFDataValidationConstraint.<init>(XSSFDataValidationConstraint.java:50)
        at org.apache.poi.xssf.usermodel.XSSFDataValidationHelper.createExplicitListConstraint(XSSFDataValidationHelper.java:66)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.addDropdownToColumn(DdsFieldServiceImpl.java:1881)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.access$000(DdsFieldServiceImpl.java:127)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl$2.afterSheetCreate(DdsFieldServiceImpl.java:1799)
        at com.alibaba.excel.write.handler.SheetWriteHandler.afterSheetCreate(SheetWriteHandler.java:37)
        at com.alibaba.excel.write.handler.chain.SheetHandlerExecutionChain.afterSheetCreate(SheetHandlerExecutionChain.java:40)
        at com.alibaba.excel.util.WriteHandlerUtils.afterSheetCreate(WriteHandlerUtils.java:98)
        at com.alibaba.excel.util.WriteHandlerUtils.afterSheetCreate(WriteHandlerUtils.java:92)
        at com.alibaba.excel.context.WriteContextImpl.initSheet(WriteContextImpl.java:206)
        at com.alibaba.excel.context.WriteContextImpl.currentSheet(WriteContextImpl.java:135)
        at com.alibaba.excel.write.ExcelBuilderImpl.addContent(ExcelBuilderImpl.java:54)
        at com.alibaba.excel.ExcelWriter.write(ExcelWriter.java:73)
        at com.alibaba.excel.ExcelWriter.write(ExcelWriter.java:50)
        at com.alibaba.excel.write.builder.ExcelWriterSheetBuilder.doWrite(ExcelWriterSheetBuilder.java:62)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.downloadWithDynamicHead(DdsFieldServiceImpl.java:1689)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.exportTemplate(DdsFieldServiceImpl.java:1565)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl$$FastClassBySpringCGLIB$$a47b2c4.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:707)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl$$EnhancerBySpringCGLIB$$69b87128.exportTemplate(<generated>)
        at com.datacyber.cyberdata.controller.dds.DdsFieldController.exportTemplate(DdsFieldController.java:230)
        at com.datacyber.cyberdata.controller.dds.DdsFieldController$$FastClassBySpringCGLIB$$870c4379.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:89)
        at com.datacyber.cyberdata.common.aspect.AESCryptoOperationAop.aesOperation(AESCryptoOperationAop.java:49)
        at jdk.internal.reflect.GeneratedMethodAccessor184.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:634)
        at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:624)
        at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:72)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:707)
        at com.datacyber.cyberdata.controller.dds.DdsFieldController$$EnhancerBySpringCGLIB$$8e2984ab.exportTemplate(<generated>)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
        at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
        at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
        at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
        at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
        at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
        at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
        at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:645)
        at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:750)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at com.datacyber.cyberdata.service.filter.LicenseRoleAuthorizationFilter.doFilterInternal(LicenseRoleAuthorizationFilter.java:79)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:111)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:337)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:115)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:81)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:122)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:116)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:126)
        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:81)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:109)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:149)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at com.datacyber.cyber.user.sdk.filter.JwtAuthenticationTokenFilter.doFilterInternal(JwtAuthenticationTokenFilter.java:83)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:111)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:103)
        at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:89)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:91)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.header.HeaderWriterFilter.doHeadersAfter(HeaderWriterFilter.java:90)
        at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:75)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:112)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:82)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:55)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.session.DisableEncodeUrlFilter.doFilterInternal(DisableEncodeUrlFilter.java:42)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:221)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:186)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at datadog.trace.instrumentation.springweb.HandlerMappingResourceNameFilter.doFilterInternal(HandlerMappingResourceNameFilter.java:50)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at com.datacyber.cyber.user.sdk.filter.ExceptionFilter.doFilterInternal(ExceptionFilter.java:22)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.ServletRequestPathFilter.doFilter(ServletRequestPathFilter.java:56)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
        at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
        at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:765)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
        at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
        at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
        at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
        at org.apache.tomcat.util.net.NioEndpoint$SocketProcesso



[2025-11-05 18:37:45.132] [traceId=54da77d1663e065339dd72a429c8ed56 spanId=752087985961214363] [service=cyber-usercenter-test ] [c.d.c.u.s.s.impl.SysDataServiceImpl] [http-nio-8080-exec-6] [ERROR] Failed to get token from /sdkAuth/getToken: 400 : "<!doctype html><html lang="en"><head><title>HTTP Status 400 – Bad Request</title><style type="text/css">body {font-family:Tahoma,Arial,sans-serif;} h1, h2, h3, b {color:white;background-color:#525D76;} h1 {font-size:22px;} h2 {font-size:16px;} h3 {font-size:14px;} p {font-size:12px;} a {color:black;} .line {height:1px;background-color:#525D76;border:none;}</style></head><body><h1>HTTP Status 400 – Bad Request</h1></body></html>"







https://cnp.anta.com/kpanda/clusters/

http://10.131.128.80:7799/api/tenant/mrs/user/export?managerUrl=https://10.131.194.230:9022/mrsmanager/&password=km72QG3Npy!1&username=u_adp_manager



https://authtest.anta.com/logout?service=https://authtest.anta.com/oauth2.0/authorize?response_type=code&client_id=100264&redirect_uri=https://adp-test.anta.com/index


[2025-09-23 10:32:42.316] [traceId=cca5ec05f94c96e4feeb01a510f4be7d spanId=748098214889791199] [service=cyber-worker-test ] [c.d.c.w.a.s.DatasourceQueryService] [http-nio-7800-exec-5] [INFO ] datasourceQuery enter: {"env":"dev","info":{"baseInputParamModelsJson":"[]","database":"poc_test","datasourceConnectionJson":"{\"catalogName\":\"doris_test\",\"collectSwitch\":1,\"connectivityInfo\":\"{\\\"connectivityStatus\\\":true,\\\"errorMessage\\\":\\\"\\\",\\\"resourceGroupId\\\":1,\\\"testTime\\\":\\\"2025-09-22T09:23:14.652\\\"}\",\"connectivityStatus\":1,\"createEngineSwitch\":1,\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"datasourceId\":1968879021349781506,\"detail\":\"{\\\"feAddress\\\":[\\\"18030\\\"],\\\"password\\\":\\\"/+PcZuJ8EnhfeBU1kxMNow==\\\",\\\"starRocksFEAddress\\\":\\\"18030\\\",\\\"url\\\":\\\"jdbc:mysql://192.168.99.9:19030/\\\",\\\"username\\\":\\\"poc_test\\\"}\",\"env\":2,\"host\":\"192.168.99.9\",\"id\":1968879021840515074,\"port\":\"19030\",\"state\":1,\"testTime\":\"2025-09-22T09:23:15\",\"updateTime\":\"2025-09-19T11:25:13\"}","datasourceJson":"{\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"customProperties\":\"[]\",\"description\":\"\",\"id\":1968879021349781506,\"isFreeze\":0,\"mode\":2,\"name\":\"doris_test\",\"state\":1,\"tenantId\":1001,\"type\":\"DORIS\",\"updateTime\":\"2025-09-19T11:25:13\",\"updateUser\":1,\"version\":0}","isFromWizardl":true,"mode":2,"outputParamModelsJson":"[{\"fieldName\":\"id\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"yearNum\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"monthNum\",\"fieldType\":\"STRING\",\"isPaging\":false,\"sampleVale\":\"\"}]","page":true,"sourceSupportEnum":"DORIS","sql":"SELECT * FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2`  ) t LIMIT 10 OFFSET 0","test":true,"totalSql":"SELECT count(*) AS total FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2` ) t","userId":48},"methodType":"API_SERVER_SQL","requestId":1758594762308,"resourceGroupId":1,"taskType":"DATASOURCE_QUERY"}









[2025-09-23 09:32:07.684] [traceId=ec22eafa680e1d9be7ac9d21e3c973d3 spanId=6230099790631525190] [service=cyber-worker-test ] [c.d.c.w.a.s.DatasourceQueryService] [http-nio-7800-exec-7] [INFO ] datasourceQuery enter: {"env":"prod","info":{"baseInputParamModelsJson":"[]","datasourceConnectionJson":"{\"catalogName\":\"doris_test\",\"collectSwitch\":1,\"connectivityInfo\":\"{\\\"connectivityStatus\\\":true,\\\"errorMessage\\\":\\\"\\\",\\\"resourceGroupId\\\":1,\\\"testTime\\\":\\\"2025-09-22T09:23:08.297\\\"}\",\"connectivityStatus\":1,\"createEngineSwitch\":1,\"createTime\":\"2025-09-19T11:25:14\",\"createUser\":1,\"datasourceId\":1968879021349781506,\"detail\":\"{\\\"feAddress\\\":[\\\"18030\\\"],\\\"password\\\":\\\"/+PcZuJ8EnhfeBU1kxMNow==\\\",\\\"starRocksFEAddress\\\":\\\"18030\\\",\\\"url\\\":\\\"jdbc:mysql://192.168.99.9:19030/\\\",\\\"username\\\":\\\"poc_test\\\"}\",\"env\":5,\"host\":\"192.168.99.9\",\"id\":1968879022507409409,\"port\":\"19030\",\"state\":1,\"testTime\":\"2025-09-22T09:23:08\",\"updateTime\":\"2025-09-19T11:25:13\"}","datasourceJson":"{\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"customProperties\":\"[]\",\"description\":\"\",\"id\":1968879021349781506,\"isFreeze\":0,\"mode\":2,\"name\":\"doris_test\",\"state\":1,\"tenantId\":1001,\"type\":\"DORIS\",\"updateTime\":\"2025-09-19T11:25:13\",\"updateUser\":1,\"version\":0}","isFromWizardl":true,"mode":2,"outputParamModelsJson":"[{\"fieldName\":\"id\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"yearNum\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"monthNum\",\"fieldType\":\"STRING\",\"isPaging\":false,\"sampleVale\":\"\"}]","page":true,"sourceSupportEnum":"DORIS","sql":"SELECT * FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2`  ) t LIMIT 10 OFFSET 0","test":true,"totalSql":"SELECT count(*) AS total FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2` ) t","userId":48},"methodType":"API_SERVER_SQL","requestId":1758591127677,"resourceGroupId":1,"taskType":"DATASOURCE_QUERY"}



[2025-09-23 09:32:07.695] [traceId=ec22eafa680e1d9be7ac9d21e3c973d3 spanId=6230099790631525190] [service=cyber-worker-test ] [c.d.c.w.a.s.DatasourceQueryService] [http-nio-7800-exec-7] [ERROR] datasourceQuery failed: param:{"connInfo":{"dataSourceType":"DORIS","datasourceConnectionId":1968879022507409409,"jdbcUrl":"jdbc:mysql://192.168.99.9:19030/","password":"poctest.321","username":"poc_test","version":0},"env":"prod","info":{"baseInputParamModelsJson":"[]","datasourceConnectionJson":"{\"catalogName\":\"doris_test\",\"collectSwitch\":1,\"connectivityInfo\":\"{\\\"connectivityStatus\\\":true,\\\"errorMessage\\\":\\\"\\\",\\\"resourceGroupId\\\":1,\\\"testTime\\\":\\\"2025-09-22T09:23:08.297\\\"}\",\"connectivityStatus\":1,\"createEngineSwitch\":1,\"createTime\":\"2025-09-19T11:25:14\",\"createUser\":1,\"datasourceId\":1968879021349781506,\"detail\":\"{\\\"feAddress\\\":[\\\"18030\\\"],\\\"password\\\":\\\"/+PcZuJ8EnhfeBU1kxMNow==\\\",\\\"starRocksFEAddress\\\":\\\"18030\\\",\\\"url\\\":\\\"jdbc:mysql://192.168.99.9:19030/\\\",\\\"username\\\":\\\"poc_test\\\"}\",\"env\":5,\"host\":\"192.168.99.9\",\"id\":1968879022507409409,\"port\":\"19030\",\"state\":1,\"testTime\":\"2025-09-22T09:23:08\",\"updateTime\":\"2025-09-19T11:25:13\"}","datasourceJson":"{\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"customProperties\":\"[]\",\"description\":\"\",\"id\":1968879021349781506,\"isFreeze\":0,\"mode\":2,\"name\":\"doris_test\",\"state\":1,\"tenantId\":1001,\"type\":\"DORIS\",\"updateTime\":\"2025-09-19T11:25:13\",\"updateUser\":1,\"version\":0}","isFromWizardl":true,"mode":2,"outputParamModelsJson":"[{\"fieldName\":\"id\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"yearNum\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"monthNum\",\"fieldType\":\"STRING\",\"isPaging\":false,\"sampleVale\":\"\"}]","page":true,"sourceSupportEnum":"DORIS","sql":"SELECT * FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2`  ) t LIMIT 10 OFFSET 0","test":true,"totalSql":"SELECT count(*) AS total FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2` ) t","userId":48},"methodType":"API_SERVER_SQL","requestId":1758591127677,"resourceGroupId":1,"taskType":"DATASOURCE_QUERY"}, ex:

java.sql.SQLException: errCode = 2, detailMessage = No database selected
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:130)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:972)
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java)
	at com.datacyber.cyberdata.worker.api.service.DatasourceQueryService.getApiServerSqlQueryResult(DatasourceQueryService.java:276)
	at com.datacyber.cyberdata.worker.api.service.DatasourceQueryService.datasourceQuery(DatasourceQueryService.java:236)
	at com.datacyber.cyberdata.worker.api.controller.DatasourceController.datasourceQuery(DatasourceController.java:39)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at datadog.trace.instrumentation.springweb.HandlerMappingResourceNameFilter.doFilterInternal(HandlerMappingResourceNameFilter.java:50)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.ServletRequestPathFilter.doFilter(ServletRequestPathFilter.java:56)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:765)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:833)
