2026-01-28 14:25:40 INFO  Current task status: Running
2026-01-28 14:25:40 INFO  Start execute FLINK_SQL on worker_host: 10.131.9.32:7800
2026-01-28 14:25:40 INFO  Current log dir: /opt/app/instanceLogs/20260128/instanceLogs/2012469866704056322/temp/2016397243777142786/1769581540620.log
2026-01-28 14:25:40 INFO  Task executor: com.datacyber.cyberdata.worker.service.realtime.FlinkSQLExecutorService
日志获取中
2026-01-28 14:25:44 INFO  Finished to download engine configs
2026-01-28 14:25:45 INFO  Run on yarn, finished to upload jars to hdfs, url:hdfs://hacluster/cyberdata/flink/lib/1.20.0-mrs360/FLINK_SQL_1080533629
2026-01-28 14:25:45 INFO  The environment is initialized
2026-01-28 14:25:45 INFO  Prepare the flink submit script
2026-01-28 14:25:46 INFO  Start download udf jars......
2026-01-28 14:25:46 INFO  End download udf jars
2026-01-28 14:25:46 INFO  Task dependent jar: []
2026-01-28 14:25:50 INFO  Submit FLINK_SQL on yarn with YARN_APPLICATION
2026-01-28 14:25:51 INFO  flink command:/opt/module/flink-1.20.0-mrs360/bin/flink run-application -t yarn-application -Dstate.backend.type=rocksdb -Drestart-strategy.type=fixeddelay -Dstate.backend.incremental=true -Dexecution.checkpointing.max-concurrent-checkpoints=3 -Dclassloader.check-leaked-classloader=false -Denv.java.opts.all=-Dfile.encoding=UTF-8 -Dpipeline.name="写入kafka流__flink_dev_temp" -Dmetrics.system-resource=true -Dmetrics.system-resource-probing-interval=30000 -Dyarn.application.queue=default -Dyarn.application.name="FLINK_SQL_写入kafka流_flink_dev_temp" -Dyarn.provided.lib.dirs=hdfs://hacluster/cyberdata/flink/lib/1.20.0-mrs360/FLINK_SQL_1080533629 -Dclassloader.resolve-order=child-first -Djobmanager.memory.process.size=1024m -Dtaskmanager.memory.process.size=2048m -Dparallelism.default=1 -Dtaskmanager.numberOfTaskSlots=1 -Dyarn.appmaster.vcores=1 -Dyarn.containers.vcores=-1 -Dexecution.checkpointing.interval=120000 -Dstate.checkpoints.dir=hdfs://hacluster/cyberdata/flink/checkpoint/2016397243777142786 -Dexecution.checkpointing.externalized-checkpoint-retention=RETAIN_ON_CANCELLATION -Dstate.checkpoints.num-retained=10 -Dexecution.checkpointing.tolerable-failed-checkpoints=3 -Dexecution.checkpointing.timeout="10min" -Drestart-strategy=fixeddelay -Drestart-strategy.fixed-delay.attempts=1 -Drestart-strategy.fixed-delay.delay="5s" -c com.datacyber.flink.FlinkSqlTask -d hdfs://hacluster/cyberdata/flink/driver/cyberdata-5.0-flink-driver-1.3.0-with-sql-flink-1.20.0-mrs360-1411420272.jar -sleepTime 20000 -logUrl http://10.131.0.85:30303/worker/writeLog -logPath /opt/app/instanceLogs/20260128/instanceLogs/2012469866704056322/temp/2016397243777142786/1769581540620.log -jobName 写入kafka流__flink_dev_temp -deployMode yarn_application -yarnMasterUrl https://10.131.194.94:9022/component/Yarn/ResourceManager/18,https://10.131.194.94:9022/component/Yarn/ResourceManager/19 -runtimeMode streaming -sqlContent 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 
2026-01-28 14:25:51 INFO  Prepare the flink script Successful
2026-01-28 14:25:51 INFO  Full Command
2026-01-28 14:25:51 INFO  -------------------------
2026-01-28 14:25:51 INFO  /bin/bash /home/datac/shell/2016397243777142786.sh
2026-01-28 14:25:51 INFO  -------------------------
2026-01-28 14:25:51 INFO  --- Invoking Shell command line now, task name:写入kafka流 ---
2026-01-28 14:25:51 INFO  =================================================================
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/flink-1.20.0-mrs360/lib/log4j-slf4j-impl-2.24.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/flink-1.20.0-mrs360/lib/flink-dist-1.20.0-h0.cbu.mrs.360.r9.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2026-01-28 14:27:12,582 | INFO  | [main] | Login successful for user p_mrs_user_test using keytab file kerberos.keytab. Keytab auto renewal enabled : false | org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1136)
2026-01-28 14:27:16,270 | INFO  | [main] | No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar | org.apache.flink.yarn.YarnClusterDescriptor.getLocalFlinkDistPath(YarnClusterDescriptor.java:285)
2026-01-28 14:27:18,100 | INFO  | [main] | resource-types.xml not found | org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream(Configuration.java:2871)
2026-01-28 14:27:18,117 | INFO  | [main] | Unable to find 'resource-types.xml'. | org.apache.hadoop.yarn.util.resource.ResourceUtils.addResourcesFileToConf(ResourceUtils.java:483)











2026-01-28 14:29:16 INFO  Current task status: Running
2026-01-28 14:29:16 INFO  Start execute FLINK_SQL on worker_host: 10.131.9.32:7800
2026-01-28 14:29:16 INFO  Current log dir: /opt/app/instanceLogs/20260128/instanceLogs/2012469866704056322/temp/2016398149012803585/1769581756336.log
2026-01-28 14:29:16 INFO  Task executor: com.datacyber.cyberdata.worker.service.realtime.FlinkSQLExecutorService
日志获取中
2026-01-28 14:29:16 INFO  Finished to download engine configs
2026-01-28 14:29:17 INFO  Run on yarn, finished to upload jars to hdfs, url:hdfs://hacluster/cyberdata/flink/lib/1.20.0-mrs360/FLINK_SQL_1080533629
2026-01-28 14:29:17 INFO  The environment is initialized
2026-01-28 14:29:17 INFO  Prepare the flink submit script
2026-01-28 14:29:17 INFO  Start download udf jars......
2026-01-28 14:29:17 INFO  End download udf jars
2026-01-28 14:29:17 INFO  Task dependent jar: []
2026-01-28 14:29:17 INFO  Submit FLINK_SQL on yarn with YARN_APPLICATION
2026-01-28 14:29:17 INFO  flink command:/opt/module/flink-1.20.0-mrs360/bin/flink run-application -t yarn-application -Dstate.backend.type=rocksdb -Drestart-strategy.type=fixeddelay -Dstate.backend.incremental=true -Dexecution.checkpointing.max-concurrent-checkpoints=3 -Dclassloader.check-leaked-classloader=false -Denv.java.opts.all=-Dfile.encoding=UTF-8 -Dpipeline.name="写入kafka流__flink_dev_temp" -Dmetrics.system-resource=true -Dmetrics.system-resource-probing-interval=30000 -Dyarn.application.queue=default -Dyarn.application.name="FLINK_SQL_写入kafka流_flink_dev_temp" -Dyarn.provided.lib.dirs=hdfs://hacluster/cyberdata/flink/lib/1.20.0-mrs360/FLINK_SQL_1080533629 -Dclassloader.resolve-order=child-first -Djobmanager.memory.process.size=1024m -Dtaskmanager.memory.process.size=2048m -Dparallelism.default=1 -Dtaskmanager.numberOfTaskSlots=1 -Dyarn.appmaster.vcores=1 -Dyarn.containers.vcores=-1 -Dexecution.checkpointing.interval=120000 -Dstate.checkpoints.dir=hdfs://hacluster/cyberdata/flink/checkpoint/2016398149012803585 -Dexecution.checkpointing.externalized-checkpoint-retention=RETAIN_ON_CANCELLATION -Dstate.checkpoints.num-retained=10 -Dexecution.checkpointing.tolerable-failed-checkpoints=3 -Dexecution.checkpointing.timeout="10min" -Drestart-strategy=fixeddelay -Drestart-strategy.fixed-delay.attempts=1 -Drestart-strategy.fixed-delay.delay="5s" -c com.datacyber.flink.FlinkSqlTask -d hdfs://hacluster/cyberdata/flink/driver/cyberdata-5.0-flink-driver-1.3.0-with-sql-flink-1.20.0-mrs360-1411420272.jar -sleepTime 20000 -logUrl http://10.131.0.85:30303/worker/writeLog -logPath /opt/app/instanceLogs/20260128/instanceLogs/2012469866704056322/temp/2016398149012803585/1769581756336.log -jobName 写入kafka流__flink_dev_temp -deployMode yarn_application -yarnMasterUrl https://10.131.194.94:9022/component/Yarn/ResourceManager/18,https://10.131.194.94:9022/component/Yarn/ResourceManager/19 -runtimeMode streaming -sqlContent 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 
2026-01-28 14:29:17 INFO  Prepare the flink script Successful
2026-01-28 14:29:17 INFO  Full Command
2026-01-28 14:29:17 INFO  -------------------------
2026-01-28 14:29:17 INFO  /bin/bash /home/datac/shell/2016398149012803585.sh
2026-01-28 14:29:17 INFO  -------------------------
2026-01-28 14:29:17 INFO  --- Invoking Shell command line now, task name:写入kafka流 ---
2026-01-28 14:29:17 INFO  =================================================================
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/flink-1.20.0-mrs360/lib/log4j-slf4j-impl-2.24.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/flink-1.20.0-mrs360/lib/flink-dist-1.20.0-h0.cbu.mrs.360.r9.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2026-01-28 14:30:25,347 | INFO  | [main] | Login successful for user p_mrs_user_test using keytab file kerberos.keytab. Keytab auto renewal enabled : false | org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1136)
2026-01-28 14:30:28,436 | INFO  | [main] | No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar | org.apache.flink.yarn.YarnClusterDescriptor.getLocalFlinkDistPath(YarnClusterDescriptor.java:285)
2026-01-28 14:30:29,730 | INFO  | [main] | resource-types.xml not found | org.apache.hadoop.conf.Configuration.getConfResourceAsInputStream(Configuration.java:2871)
2026-01-28 14:30:29,768 | INFO  | [main] | Unable to find 'resource-types.xml'. | org.apache.hadoop.yarn.util.resource.ResourceUtils.addResourcesFileToConf(ResourceUtils.java:483)






































-- ================================================================
-- 完整SQL：新权限点 DataQualityDataComparisonTask:manualExecution
-- 包含: biz_permission + biz_permission_resource + biz_role_permission
-- ================================================================
-- ====== 步骤1：插入 biz_permission 权限点 ======
INSERT INTO biz_permission (parent_id, app_code, permission_code, path_code, level, name, type, sort, is_system, description, state, create_user, create_time)
SELECT 
    null AS parent_id,
    p.app_code,
    'DataQualityRuleManage' AS permission_code,
    null AS path_code,
    0 AS level,
    '质量规则' AS name,
    3 AS type,              -- 3=按钮/操作
    (SELECT COALESCE(MAX(sort), 0) + 1 FROM biz_permission WHERE parent_id = p.id) AS sort,
    1 AS is_system,
    '质量规则' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityDataComparisonTask'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission 
      WHERE permission_code = 'DataQualityRuleManage'
  );
-- ====== 步骤2：插入 biz_permission_resource 权限资源关联 ======
-- 参考 DataQualityDataComparisonTask:add 的资源配置
INSERT INTO biz_permission_resource (
    permission_id,
    app_code,
    backend_url,
    backend_method,
    match_mode,
    frontend_path,
    element_type,
    element_key,
    sort,
    description,
    state,
    create_user,
    create_time
)
SELECT 
    p.id AS permission_id,
    p.app_code,
    NULL AS backend_url,           -- 如有具体接口URL请替换
    NULL AS backend_method,        -- 如 POST/GET 等
    1 AS match_mode,               -- 1=精确匹配
    NULL AS frontend_path,
    2 AS element_type,             -- 2=按钮
    'DataQualityRuleManage' AS element_key,  -- 前端元素标识
    1 AS sort,
    '数据比对任务手动运行按钮' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityRuleManage'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission_resource 
      WHERE permission_id = p.id AND element_key = 'DataQualityRuleManage'
  );
-- ====== 步骤3：插入 biz_role_permission 角色权限关联 ======
INSERT INTO biz_role_permission (role_id, permission_id, app_code, create_user, create_time)
SELECT 
    r.id AS role_id,
    p.id AS permission_id,
    p.app_code,
    1 AS create_user,
    NOW() AS create_time
FROM biz_role r
CROSS JOIN biz_permission p
WHERE r.name IN ('CyberDataAdmin', 'ProjectAdmin', 'Dev', 'OPS')
  AND p.permission_code = 'DataQualityRuleManage'
  AND NOT EXISTS (
      SELECT 1 FROM biz_role_permission rp 
      WHERE rp.role_id = r.id AND rp.permission_id = p.id
  );



-- ================================================================
-- 完整SQL：新权限点 DataQualityDataComparisonTask:manualExecution
-- 包含: biz_permission + biz_permission_resource + biz_role_permission
-- ================================================================
-- ====== 步骤1：插入 biz_permission 权限点 ======
INSERT INTO biz_permission (parent_id, app_code, permission_code, path_code, level, name, type, sort, is_system, description, state, create_user, create_time)
SELECT 
    null AS parent_id,
    p.app_code,
    'DataQualityRuleTag' AS permission_code,
    null AS path_code,
    0 AS level,
    '标签管理' AS name,
    3 AS type,              -- 3=按钮/操作
    (SELECT COALESCE(MAX(sort), 0) + 1 FROM biz_permission WHERE parent_id = p.id) AS sort,
    1 AS is_system,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityDataComparisonTask'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission 
      WHERE permission_code = 'DataQualityRuleTag'
  );
-- ====== 步骤2：插入 biz_permission_resource 权限资源关联 ======
-- 参考 DataQualityDataComparisonTask:add 的资源配置
INSERT INTO biz_permission_resource (
    permission_id,
    app_code,
    backend_url,
    backend_method,
    match_mode,
    frontend_path,
    element_type,
    element_key,
    sort,
    description,
    state,
    create_user,
    create_time
)
SELECT 
    p.id AS permission_id,
    p.app_code,
    NULL AS backend_url,           -- 如有具体接口URL请替换
    NULL AS backend_method,        -- 如 POST/GET 等
    1 AS match_mode,               -- 1=精确匹配
    NULL AS frontend_path,
    2 AS element_type,             -- 2=按钮
    'DataQualityRuleTag' AS element_key,  -- 前端元素标识
    1 AS sort,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityRuleTag'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission_resource 
      WHERE permission_id = p.id AND element_key = 'DataQualityRuleTag'
  );
-- ====== 步骤3：插入 biz_role_permission 角色权限关联 ======
INSERT INTO biz_role_permission (role_id, permission_id, app_code, create_user, create_time)
SELECT 
    r.id AS role_id,
    p.id AS permission_id,
    p.app_code,
    1 AS create_user,
    NOW() AS create_time
FROM biz_role r
CROSS JOIN biz_permission p
WHERE r.name IN ('CyberDataAdmin', 'ProjectAdmin', 'Dev', 'OPS')
  AND p.permission_code = 'DataQualityRuleTag'
  AND NOT EXISTS (
      SELECT 1 FROM biz_role_permission rp 
      WHERE rp.role_id = r.id AND rp.permission_id = p.id
  );
	


-- ================================================================
-- 完整SQL：新权限点 DataQualityDataComparisonTask:manualExecution
-- 包含: biz_permission + biz_permission_resource + biz_role_permission
-- ================================================================
-- ====== 步骤1：插入 biz_permission 权限点 ======
INSERT INTO biz_permission (parent_id, app_code, permission_code, path_code, level, name, type, sort, is_system, description, state, create_user, create_time)
SELECT 
    null AS parent_id,
    p.app_code,
    'DataQualityRuleManage:add' AS permission_code,
    null AS path_code,
    0 AS level,
    '标签管理' AS name,
    3 AS type,              -- 3=按钮/操作
    (SELECT COALESCE(MAX(sort), 0) + 1 FROM biz_permission WHERE parent_id = p.id) AS sort,
    1 AS is_system,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityDataComparisonTask'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission 
      WHERE permission_code = 'DataQualityRuleManage:add'
  );
-- ====== 步骤2：插入 biz_permission_resource 权限资源关联 ======
-- 参考 DataQualityDataComparisonTask:add 的资源配置
INSERT INTO biz_permission_resource (
    permission_id,
    app_code,
    backend_url,
    backend_method,
    match_mode,
    frontend_path,
    element_type,
    element_key,
    sort,
    description,
    state,
    create_user,
    create_time
)
SELECT 
    p.id AS permission_id,
    p.app_code,
    NULL AS backend_url,           -- 如有具体接口URL请替换
    NULL AS backend_method,        -- 如 POST/GET 等
    1 AS match_mode,               -- 1=精确匹配
    NULL AS frontend_path,
    2 AS element_type,             -- 2=按钮
    'DataQualityRuleManage:add' AS element_key,  -- 前端元素标识
    1 AS sort,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityRuleManage:add'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission_resource 
      WHERE permission_id = p.id AND element_key = 'DataQualityRuleManage:add'
  );
-- ====== 步骤3：插入 biz_role_permission 角色权限关联 ======
INSERT INTO biz_role_permission (role_id, permission_id, app_code, create_user, create_time)
SELECT 
    r.id AS role_id,
    p.id AS permission_id,
    p.app_code,
    1 AS create_user,
    NOW() AS create_time
FROM biz_role r
CROSS JOIN biz_permission p
WHERE r.name IN ('CyberDataAdmin', 'ProjectAdmin', 'Dev', 'OPS')
  AND p.permission_code = 'DataQualityRuleManage:add'
  AND NOT EXISTS (
      SELECT 1 FROM biz_role_permission rp 
      WHERE rp.role_id = r.id AND rp.permission_id = p.id
  );



-- ================================================================
-- 完整SQL：新权限点 DataQualityDataComparisonTask:manualExecution
-- 包含: biz_permission + biz_permission_resource + biz_role_permission
-- ================================================================
-- ====== 步骤1：插入 biz_permission 权限点 ======
INSERT INTO biz_permission (parent_id, app_code, permission_code, path_code, level, name, type, sort, is_system, description, state, create_user, create_time)
SELECT 
    null AS parent_id,
    p.app_code,
    'DataQualityRuleManage:edit' AS permission_code,
    null AS path_code,
    0 AS level,
    '标签管理' AS name,
    3 AS type,              -- 3=按钮/操作
    (SELECT COALESCE(MAX(sort), 0) + 1 FROM biz_permission WHERE parent_id = p.id) AS sort,
    1 AS is_system,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityDataComparisonTask'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission 
      WHERE permission_code = 'DataQualityRuleManage:edit'
  );
-- ====== 步骤2：插入 biz_permission_resource 权限资源关联 ======
-- 参考 DataQualityDataComparisonTask:add 的资源配置
INSERT INTO biz_permission_resource (
    permission_id,
    app_code,
    backend_url,
    backend_method,
    match_mode,
    frontend_path,
    element_type,
    element_key,
    sort,
    description,
    state,
    create_user,
    create_time
)
SELECT 
    p.id AS permission_id,
    p.app_code,
    NULL AS backend_url,           -- 如有具体接口URL请替换
    NULL AS backend_method,        -- 如 POST/GET 等
    1 AS match_mode,               -- 1=精确匹配
    NULL AS frontend_path,
    2 AS element_type,             -- 2=按钮
    'DataQualityRuleManage:edit' AS element_key,  -- 前端元素标识
    1 AS sort,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityRuleManage:edit'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission_resource 
      WHERE permission_id = p.id AND element_key = 'DataQualityRuleManage:edit'
  );
-- ====== 步骤3：插入 biz_role_permission 角色权限关联 ======
INSERT INTO biz_role_permission (role_id, permission_id, app_code, create_user, create_time)
SELECT 
    r.id AS role_id,
    p.id AS permission_id,
    p.app_code,
    1 AS create_user,
    NOW() AS create_time
FROM biz_role r
CROSS JOIN biz_permission p
WHERE r.name IN ('CyberDataAdmin', 'ProjectAdmin', 'Dev', 'OPS')
  AND p.permission_code = 'DataQualityRuleManage:edit'
  AND NOT EXISTS (
      SELECT 1 FROM biz_role_permission rp 
      WHERE rp.role_id = r.id AND rp.permission_id = p.id
  );


-- ================================================================
-- 完整SQL：新权限点 DataQualityDataComparisonTask:manualExecution
-- 包含: biz_permission + biz_permission_resource + biz_role_permission
-- ================================================================
-- ====== 步骤1：插入 biz_permission 权限点 ======
INSERT INTO biz_permission (parent_id, app_code, permission_code, path_code, level, name, type, sort, is_system, description, state, create_user, create_time)
SELECT 
    null AS parent_id,
    p.app_code,
    'DataQualityRuleManage:delete' AS permission_code,
    null AS path_code,
    0 AS level,
    '标签管理' AS name,
    3 AS type,              -- 3=按钮/操作
    (SELECT COALESCE(MAX(sort), 0) + 1 FROM biz_permission WHERE parent_id = p.id) AS sort,
    1 AS is_system,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityDataComparisonTask'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission 
      WHERE permission_code = 'DataQualityRuleManage:delete'
  );
-- ====== 步骤2：插入 biz_permission_resource 权限资源关联 ======
-- 参考 DataQualityDataComparisonTask:add 的资源配置
INSERT INTO biz_permission_resource (
    permission_id,
    app_code,
    backend_url,
    backend_method,
    match_mode,
    frontend_path,
    element_type,
    element_key,
    sort,
    description,
    state,
    create_user,
    create_time
)
SELECT 
    p.id AS permission_id,
    p.app_code,
    NULL AS backend_url,           -- 如有具体接口URL请替换
    NULL AS backend_method,        -- 如 POST/GET 等
    1 AS match_mode,               -- 1=精确匹配
    NULL AS frontend_path,
    2 AS element_type,             -- 2=按钮
    'DataQualityRuleManage:delete' AS element_key,  -- 前端元素标识
    1 AS sort,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityRuleManage:delete'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission_resource 
      WHERE permission_id = p.id AND element_key = 'DataQualityRuleManage:delete'
  );
-- ====== 步骤3：插入 biz_role_permission 角色权限关联 ======
INSERT INTO biz_role_permission (role_id, permission_id, app_code, create_user, create_time)
SELECT 
    r.id AS role_id,
    p.id AS permission_id,
    p.app_code,
    1 AS create_user,
    NOW() AS create_time
FROM biz_role r
CROSS JOIN biz_permission p
WHERE r.name IN ('CyberDataAdmin', 'ProjectAdmin', 'Dev', 'OPS')
  AND p.permission_code = 'DataQualityRuleManage:delete'
  AND NOT EXISTS (
      SELECT 1 FROM biz_role_permission rp 
      WHERE rp.role_id = r.id AND rp.permission_id = p.id
  );


-- ================================================================
-- 完整SQL：新权限点 DataQualityDataComparisonTask:manualExecution
-- 包含: biz_permission + biz_permission_resource + biz_role_permission
-- ================================================================
-- ====== 步骤1：插入 biz_permission 权限点 ======
INSERT INTO biz_permission (parent_id, app_code, permission_code, path_code, level, name, type, sort, is_system, description, state, create_user, create_time)
SELECT 
    null AS parent_id,
    p.app_code,
    'DataQualityRuleManage:detail' AS permission_code,
    null AS path_code,
    0 AS level,
    '标签管理' AS name,
    3 AS type,              -- 3=按钮/操作
    (SELECT COALESCE(MAX(sort), 0) + 1 FROM biz_permission WHERE parent_id = p.id) AS sort,
    1 AS is_system,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityDataComparisonTask'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission 
      WHERE permission_code = 'DataQualityRuleManage:detail'
  );
-- ====== 步骤2：插入 biz_permission_resource 权限资源关联 ======
-- 参考 DataQualityDataComparisonTask:add 的资源配置
INSERT INTO biz_permission_resource (
    permission_id,
    app_code,
    backend_url,
    backend_method,
    match_mode,
    frontend_path,
    element_type,
    element_key,
    sort,
    description,
    state,
    create_user,
    create_time
)
SELECT 
    p.id AS permission_id,
    p.app_code,
    NULL AS backend_url,           -- 如有具体接口URL请替换
    NULL AS backend_method,        -- 如 POST/GET 等
    1 AS match_mode,               -- 1=精确匹配
    NULL AS frontend_path,
    2 AS element_type,             -- 2=按钮
    'DataQualityRuleManage:detail' AS element_key,  -- 前端元素标识
    1 AS sort,
    '标签管理' AS description,
    1 AS state,
    1 AS create_user,
    NOW() AS create_time
FROM biz_permission p
WHERE p.permission_code = 'DataQualityRuleManage:detail'
  AND NOT EXISTS (
      SELECT 1 FROM biz_permission_resource 
      WHERE permission_id = p.id AND element_key = 'DataQualityRuleManage:detail'
  );
-- ====== 步骤3：插入 biz_role_permission 角色权限关联 ======
INSERT INTO biz_role_permission (role_id, permission_id, app_code, create_user, create_time)
SELECT 
    r.id AS role_id,
    p.id AS permission_id,
    p.app_code,
    1 AS create_user,
    NOW() AS create_time
FROM biz_role r
CROSS JOIN biz_permission p
WHERE r.name IN ('CyberDataAdmin', 'ProjectAdmin', 'Dev', 'OPS')
  AND p.permission_code = 'DataQualityRuleManage:detail'
  AND NOT EXISTS (
      SELECT 1 FROM biz_role_permission rp 
      WHERE rp.role_id = r.id AND rp.permission_id = p.id
  );

























2026-01-22 15:08:30 INFO  Current task status: Running
2026-01-22 15:08:30 INFO  Start execute HIVE on worker_host: 10.131.8.19:7800
2026-01-22 15:08:30 INFO  Current log dir: /opt/app/instanceLogs/20260122/instanceLogs/2012887764074090498/temp/2014233698000322561/1769065710945.log
2026-01-22 15:08:31 INFO  Task executor: com.datacyber.cyberdata.worker.service.JdbcSQLExecutorService
2026-01-22 15:08:31 INFO  kerberos principal:u_adp_proxyuser
2026-01-22 15:08:31 INFO  Start execute jdbc task:111,taskID:2013435106136399874
2026-01-22 15:08:31 INFO  get jdbc plugin:com.datacyber.cybermeta.jdbc.plugins.HiveMrsPlugin$HiveJdbcDialect@5dddb1dc
2026-01-22 15:08:31 INFO  Hive Start execution
2026-01-22 15:08:31 INFO  Start create connection
2026-01-22 15:08:31 INFO  Create connection complete
2026-01-22 15:08:31 INFO  Start log thread
2026-01-22 15:08:31 INFO  start execute sql:show tables
2026-01-22 15:08:31 INFO  execute result column:[SQL:show tables, Copy:true, tab_name]
2026-01-22 15:08:31 INFO  appId:null
2026-01-22 15:08:31 INFO  HIVE Execution complete
2026-01-22 15:08:31 INFO  excutorContext.InstanceState: SUCCEED
2026-01-22 15:08:31 INFO  Final task status: Run successfully
2026-01-22 15:08:31 INFO  Cost time is: 0.741s
2026-01-22 15:08:31 INFO  END-EOF






2026-01-22 15:07:52 INFO  Current task status: Running
2026-01-22 15:07:52 INFO  Start execute HIVE on worker_host: 10.131.8.19:7800
2026-01-22 15:07:52 INFO  Current log dir: /opt/app/instanceLogs/20260122/instanceLogs/2012887764074090498/temp/2014233537912127490/1769065672779.log
2026-01-22 15:07:52 INFO  Task executor: com.datacyber.cyberdata.worker.service.JdbcSQLExecutorService
2026-01-22 15:07:53 INFO  kerberos principal:u_adp_proxyuser
2026-01-22 15:07:53 INFO  Start execute jdbc task:111,taskID:2013435106136399874
2026-01-22 15:07:53 INFO  get jdbc plugin:com.datacyber.cybermeta.jdbc.plugins.HiveMrsPlugin$HiveJdbcDialect@207f0aec
2026-01-22 15:07:53 INFO  Hive Start execution
2026-01-22 15:07:53 INFO  Start create connection
2026-01-22 15:07:53 ERROR sqlErr:Authentication: kerberos 登录失败: null
java.lang.RuntimeException: Authentication: kerberos 登录失败: null
	at com.datacyber.cybermeta.jdbc.plugins.HiveMrsPlugin$HiveJdbcDialect.runSecured(HiveMrsPlugin.java:99)
	at com.datacyber.cybermeta.jdbc.plugins.HiveMrsPlugin$HiveJdbcDialect.getConnection(HiveMrsPlugin.java:220)
	at com.datacyber.cyberdata.worker.service.JdbcSQLExecutorService.executeTask(JdbcSQLExecutorService.java:259)
	at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor.runTask(AbstractTaskExecutor.java:309)
	at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor$$FastClassBySpringCGLIB$$4b20b1cc.invoke(&lt;generated&gt;)
	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
	at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.reflect.UndeclaredThrowableException: null
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1894)
	at com.datacyber.cybermeta.jdbc.plugins.HiveMrsPlugin$HiveJdbcDialect.runSecured(HiveMrsPlugin.java:95)
	... 16 common frames omitted
Caused by: java.sql.SQLException: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper
	at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:170)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
	at com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactoryNoPool.openConnection(DriverConnectionFactoryNoPool.java:50)
	at com.datacyber.cybermeta.jdbc.plugins.HiveMrsPlugin$HiveJdbcDialect.getHiveConnection(HiveMrsPlugin.java:225)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	... 17 common frames omitted
Caused by: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper
	at org.apache.hive.jdbc.ZooKeeperHiveClientHelper.configureConnParams(ZooKeeperHiveClientHelper.java:147)
	at org.apache.hive.jdbc.Utils.configureConnParamsFromZooKeeper(Utils.java:511)
	at org.apache.hive.jdbc.Utils.parseURL(Utils.java:334)
	at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:168)
	... 25 common frames omitted
Caused by: org.apache.zookeeper.KeeperException$SessionClosedRequireAuthException: KeeperErrorCode = Session closed because client failed to authenticate for /hiveserver2
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:153)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:54)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:2869)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:242)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:231)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:93)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:228)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:219)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:41)
	at org.apache.hive.jdbc.ZooKeeperHiveClientHelper.getServerHosts(ZooKeeperHiveClientHelper.java:98)
	at org.apache.hive.jdbc.ZooKeeperHiveClientHelper.configureConnParams(ZooKeeperHiveClientHelper.java:142)
	... 28 common frames omitted
2026-01-22 15:07:53 INFO  excutorContext.InstanceState: FAILED_ERROR
2026-01-22 15:07:53 INFO  Task failed status: Error failed
2026-01-22 15:07:53 INFO  Final task status: Error failed
2026-01-22 15:07:53 INFO  Cost time is: 0.659s
2026-01-22 15:07:53 INFO  END-EOF
































2026-01-15 20:37:47.062 ERROR [cyber-worker] [FLINK_SQL-TaskExecutor-2011779845563838465] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - 未找到引擎集群: clusterName=FLINK_IAM_DEV, tenantId=1001
2026-01-15 20:37:47.315 ERROR [cyber-worker] [FLINK_SQL-TaskExecutor-2011779845563838465] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - Task execute failed 
com.datacyber.cyberdata.worker.exception.WorkerException: you must bind kerberos in this cluster, please check flink engine config in project
        at com.datacyber.cyberdata.worker.service.WorkerService.downloadEngineConf(WorkerService.java:360)
        at com.datacyber.cyberdata.worker.service.WorkerService.initEngineEnv(WorkerService.java:174)
        at com.datacyber.cyberdata.worker.service.realtime.AbstractRealtimeExecutorService.init(AbstractRealtimeExecutorService.java:177)
        at com.datacyber.cyberdata.worker.service.ShellExecutorService.executeTask(ShellExecutorService.java:75)
        at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor.runTask(AbstractTaskExecutor.java:310)
        at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor$$FastClassBySpringCGLIB$$4b20b1cc.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-15 20:37:47.316 INFO  [cyber-worker] [FLINK_SQL-TaskExecutor-2011779845563838465] com.datacyber.cyberdata.worker.service.AbstractTask


















bd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf","principal":"u_meta","tempKerberos":true},"loginType":4,"state":1,"storageClusterId":1992956917615718402,"testTime":"2026-01-14T21:14:16","updateTime":"2026-01-14T21:10:51"}, kerberosConfName=u_meta
2026-01-15 19:04:07.005 INFO  [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - detail信息:{}
2026-01-15 19:04:07.005 INFO  [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-15 19:04:07.006 INFO  [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - kerberosInfo enabled: true, principal: u_meta
2026-01-15 19:04:07.648 WARN  [cyber-worker] [work-async-executor-3] com.datacyber.cybermeta.jdbc.plugins.jdbc.util.JsonUtils - jackson-module-scala not exist
2026-01-15 19:04:07.912 ERROR [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: Illegal base64 character 5f
        at com.datacyber.cyberdata.common.utils.KerberosUtils.keytabBytes(KerberosUtils.java:255)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.buildConnectionInfo(HBaseAccessService.java:205)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:104)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: Illegal base64 character 5f
        at java.base/java.util.Base64$Decoder.decode0(Base64.java:746)
        at java.base/java.util.Base64$Decoder.decode(Base64.java:538)
        at java.base/java.util.Base64$Decoder.decode(Base64.java:561)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.keytabBytes(KerberosUtils.java:251)
        ... 16 common frames omitted



































2026-01-15 18:04:31.414 ERROR [cyber-worker] [work-async-executor-6] com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect - runSecuredErr:executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
java.lang.RuntimeException: executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:339)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:248)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.lambda$getSchemaTables$6(HbaseMrsPlugin.java:205)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:91)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.getSchemaTables(HbaseMrsPlugin.java:205)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseMrsWithKerberos$7(DataSourceConnectionHelper.java:215)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseMrsWithKerberos$8(DataSourceConnectionHelper.java:214)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseMrsWithKerberos(DataSourceConnectionHelper.java:212)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.phoenix.exception.PhoenixIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.phoenix.util.ClientUtil.parseServerException(ClientUtil.java:69)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1806)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:2267)
        at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable(DelegateConnectionQueryServices.java:159)
        at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:3278)
        at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1064)
        at org.apache.phoenix.compile.CreateTableCompiler$CreateTableMutationPlan.execute(CreateTableCompiler.java:491)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:573)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:538)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:537)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:525)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:2244)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3657)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:272)
        at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:150)
        at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:229)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactoryNoPool.openConnection(DriverConnectionFactoryNoPool.java:50)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:260)
        ... 30 common frames omitted
Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.lambda$getTimerTask$0(ReadOnlyZKClient.java:285)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.run(HashedWheelTimer.java:713)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.ImmediateExecutor.execute(ImmediateExecutor.java:34)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:701)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:788)
        at org.apache.hbase.thirdparty.io.netty.util.Hashe
































t = 90000
2026-01-15 18:02:52.024 ERROR [cyber-worker] [ReadOnlyZKClient-10.131.194.252:2181,10.131.195.229:2181,10.131.195.79:2181@0x634d4eaf-SendThread(10.131.194.252:2181)] org.apache.zookeeper.client.ZooKeeperSaslClient - An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)]) occurred when evaluating Zookeeper Quorum Member's  received SASL token. Zookeeper Client will go to AUTH_FAILED state.
2026-01-15 18:02:52.024 ERROR [cyber-worker] [ReadOnlyZKClient-10.131.194.252:2181,10.131.195.229:2181,10.131.195.79:2181@0x634d4eaf-SendThread(10.131.194.252:2181)] org.apache.zookeeper.ClientCnxn - SASL authentication with Zookeeper Quorum member failed.
javax.security.sasl.SaslException: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)]) occurred when evaluating Zookeeper Quorum Member's  received SASL token. Zookeeper Client will go to AUTH_FAILED state.
        at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslToken(ZooKeeperSaslClient.java:338)
        at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslToken(ZooKeeperSaslClient.java:303)
        at org.apache.zookeeper.client.ZooKeeperSaslClient.sendSaslPacket(ZooKeeperSaslClient.java:366)
        at org.apache.zookeeper.client.ZooKeeperSaslClient.initialize(ZooKeeperSaslClient.java:403)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1220)
Caused by: java.security.PrivilegedActionException: null
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.zookeeper.client.ZooKeeperSaslClient.createSaslToken(ZooKeeperSaslClient.java:317)
        ... 4 common frames omitted
Caused by: javax.security.sasl.SaslException: GSS initiate failed
        at jdk.security.jgss/com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:222)
        at org.apache.zookeeper.client.ZooKeeperSaslClient$1.run(ZooKeeperSaslClient.java:320)
        at org.apache.zookeeper.client.ZooKeeperSaslClient$1.run(ZooKeeperSaslClient.java:317)
        ... 7 common frames omitted
Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)
        at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:773)
        at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:266)
        at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:196)
        at jdk.security.jgss/com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:203)
        ... 9 common frames omitted
Caused by: sun.security.krb5.KrbException: Server not found in Kerberos database (7) - LOOKING_UP_SERVER
        at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73)
        at java.security.jgss/sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:226)
        at java.security.jgss/sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:237)
        at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCredsSingle(CredentialsUtil.java:477)
        at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:340)
        at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:314)
        at java.security.jgss/sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:169)
        at java.security.jgss/sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:490)
        at java.security.jgss/sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:697)
        ... 12 common frames omitted
Caused by: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)
        at java.security.jgss/sun.security.krb5.internal.KDCRep.init(KDCRep.java:140)
        at java.security.jgss/sun.security.krb5.internal.TGSRep.init(TGSRep.java:65)
        at java.security.jgss/sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60)
        at java.security.jgss/sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55)
        ... 20 common frames omitted
2026-01-15 18:02:52.024 INFO  [cyber-worker] [ReadOnlyZKClient-10.131.194.252:2181,10.131.195.229:2181,10.131.195.79:2181@0x634d4eaf-EventThread] org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x1700006f6529ecd9
2026-01-15 18:02:54.974 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-15 18:03:00.000 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.scheduler.WorkerSelfRegistry - HealthCheckEnable=true
2026-01-15 18:03:00.033 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.api.service.impl.WorkerRpcServiceImpl - Heartbeat report url:http://coordinator-inner-dev:7500/coordinator/worker/heartbeat, request={"active":true,"availableThread":63,"cpuUsage":0.26,"createTime":"2026-01-15T17:53:37.517640000","env":"all","host":"10.131.9.118","jvmFreeMemory":4602356720,"jvmMaxMemory":6442450944,"maxCpu":3.0,"maxThread":64,"port":7800,"reservedJvmMb":7168.0,"resourceGroupNameEn":"default","sysFreeMemory":7608115200,"sysMaxMemory":12884901888,"type":"worker","updateTime":"2026-01-15T18:03:00.010821000","usedCpu":0.0,"usedJvmMb":0.0,"usedMemMb":0.0}, response={"code":"200","data":true,"msg":"成功"}




:10:29.443 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-15 17:10:29.443 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - kerberosInfo enabled: true, principal: u_meta
2026-01-15 17:10:29.467 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cyberdata.common.utils.KerberosUtils - Set JAAS config for ZooKeeper SASL: /home/datac/tmp/kerberos/jaas_d228607ca10f107299335db3c0957af0.conf
2026-01-15 17:10:29.481 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect - create no pool connection factory
2026-01-15 17:10:29.481 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cybermeta.jdbc.plugins.LoginUtil - start setJaasConf,loginContextName:Client,principal:u_meta,keytabFile:/home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.keytab
2026-01-15 17:10:29.481 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cybermeta.jdbc.plugins.LoginUtil - JaasConfiguration loginContextName=Client thePrincipal=u_meta useTicketCache=false keytabFile=/home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.keytab
2026-01-15 17:10:29.481 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect - ZOOKEEPER_DEFAULT_SERVER_PRINCIPAL:zookeeper/hadoop.b2d05ee0_e921_40a8_a6fd_dcb7693b106b.com@b2d05ee0_e921_40a8_a6fd_dcb7693b106b.com
2026-01-15 17:10:29.481 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cybermeta.jdbc.plugins.LoginUtil - setZookeeperServerPrincipal zkServerPrincipalKey=zookeeper.server.principal zkServerPrincipal=zookeeper/hadoop.b2d05ee0_e921_40a8_a6fd_dcb7693b106b.com@b2d05ee0_e921_40a8_a6fd_dcb7693b106b.com
2026-01-15 17:10:29.481 INFO  [cyber-worker] [work-async-executor-16] com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect - hbase loginToKerberos,userKeytabFile:/home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.keytab,krb5File:/home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf,principal:u_meta,coreSitePath:null,hdfsSitePath:null,hbaseSitePath:null
2026-01-15 17:10:29.481 ERROR [cyber-worker] [work-async-executor-16] com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect - runSecuredErr:Can not create a Path from a null string
java.lang.IllegalArgumentException: Can not create a Path from a null string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:168)
        at org.apache.hadoop.fs.Path.<init>(Path.java:184)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.loginToKerberos(HbaseMrsPlugin.java:118)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:85)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.createConnectionFactory(HbaseMrsPlugin.java:154)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.initConnection(AbstractJdbcDialect.java:55)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.initConnection(HbaseMrsPlugin.java:64)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.getHbaseMrsJdbcDialectWithKerberos(DataSourceConnectionHelper.java:241)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseMrsWithKerberos$8(DataSourceConnectionHelper.java:213)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseMrsWithKerberos(DataSourceConnectionHelper.java:212)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-15 17:10:29.482 ERROR [cyber-worker] [work-async-executor-16] com.datacyber.cyberdata.common.utils.KerberosUtils - Authentication: kerberos 操作失败: Can not create a Path from a null string
java.lang.RuntimeException: Authentication: kerberos 操作失败: Can not create a Path from a null string
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:95)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.createConnectionFactory(HbaseMrsPlugin.java:154)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.initConnection(AbstractJdbcDialect.java:55)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.initConnection(HbaseMrsPlugin.java:64)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.getHbaseMrsJdbcDialectWithKerberos(DataSourceConnectionHelper.java:241)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseMrsWithKerberos$8(DataSourceConnectionHelper.java:213)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseMrsWithKerberos(DataSourceConnectionHelper.java:212)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.IllegalArgumentException: Can not create a Path from a null string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:168)
        at org.apache.hadoop.fs.Path.<init>(Path.java:184)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.loginToKerberos(HbaseMrsPlugin.java:118)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:85)
        ... 25 common frames omitted
2026-01-15 17:10:29.482 ERROR [cyber-worker] [work-async-executor-16] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: Authentication: kerberos 操作失败: Can not create a Path from a null string
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:105)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseMrsWithKerberos(DataSourceConnectionHelper.java:212)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-15 17:10:29.482 ERROR [cyber-worker] [work-async-executor-16] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - collect metadata has exception, recordId:63822com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: Authentication: kerberos 操作失败: Can not create a Path from a null string



















@Object[][
        @String[{"master.rmi.RMIClientSocketFactory.class":"com.huawei.bigdata.om.controller.api.extern.monitor.RmiClientLocalhostSocketFactory","phoenix.schema.mapSystemTablesToNamespace":"true","hbase.status.publish.period":"10000","hbase.master.loadbalance.bytable":"true","hbase.ipc.server.metacallqueue.read.ratio":"0.5","dfs.replication":"3","hbase.master.port":"16000","hbase.replication.sync.table.schema.on.delete":"false","hbase.hmaster.ip.lists":"mrs-adp-q02-node-master3qkoz.mrs-h69n.com,mrs-adp-q02-node-master2femp.mrs-h69n.com","hbase.client.primaryCallTimeout.get":"10000","hbase.server":"HBase","hbase.dfs.client.read.shortcircuit.buffer.size":"131072","hbase.ipc.server.hotregion.max.callqueue.length":"330","hbase.fileStream.cleaner.tmp.thread.wakefrequency":"3600","hbase.regionserver.wal.codec":"org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec","hbase.client.zookeeper.property.clientPort":"2181","hbase.hmaster.hfilecleaner.trash.enabled":"true","hbase.client.scanner.timeout.period":"360000","hbase.large.object.threshold.max":"10485760","hbase.rootdir":"hdfs://hacluster/hbase","hbase.coprocessor.enabled":"true","hfile.format.version":"3","hbase.regionserver.hfilecleaner.large.queue.size":"10240","hbase.security.authentication":"kerberos","hbase.master.cleaner.interval":"60000","hbase.splitlog.manager.timeout":"600000","hbase.gsi.max.index.count.per.table":"5","hbase.snapshot.enabled":"true","zookeeper.session.timeout":"90000","phoenix.coprocessor.maxServerCacheTimeToLiveMs":"1800000","jmx.include.hosts":".*","hbase.quota.refresh.period":"300000","hbase.replication.sync.table.schema.on.create":"false","hbase.auth.key.update.interval":"86400000","hbase.security.authentication.spnego.admin.groups":"hbase,supergroup,System_administrator_186","hbase.master.balancer.stochastic.maxRunningTime":"300000","hbase.regionserver.hfilecleaner.small.thread.count":"5","hbase.crypto.master.key.name":"omm","hbase.huawei.restore.tmpdir":"hdfs://hacluster/hbase/extdata/restore/tmp","hbase.cleaner.scan.dir.concurrent.size":"0.5","hbase.ipc.server.callqueue.handler.factor":"0.1","hbase.fileStream.cleaner.ttl.expire.time":"604800","hbase.ssl.enabled":"true","hbase.metric.controller.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.master.namespace.init.timeout":"3600000","hbase.http.servlet.cookie.samesite":"Strict","hbase.crypto.keyprovider.parameters":"?encryptedtext=","hbase.master.truncate.fix.inconsistent":"true","hbase.crypto.wal.algorithm":"AES","hbase.assignment.retry.immediately.maximum.attempts":"3","hbase.huawei.backup.output":"/user/hbase/hbaseBackup/hbase","default.hbase.superuser":"hbase,@supergroup,@zkclient,@System_administrator_186","hbase.client.replicaCallTimeout.scan":"1000000","hbase.cold.rootdir":"/hbase","zookeeper.registry.async.get.timeout":"30000","hbase.hmaster.hfilecleaner.force.delete":"true","hbase.crypto.keyprovider.parameters.encryptedtext":"","hbase.util.ip.to.rack.determiner":"org.apache.hadoop.net.ScriptBasedMapping","hbase.config.crypt.class":"com.huawei.hadoop.datasight.security.FMHbaseCryptAdapter","hbase.data.rootdir":"/hbase","hbase.client.retries.number":"35","hbase.netty.worker.count":"0","hbase.master.balancer.decision.buffer.enabled":"true","hbase.huawei.restore.output":"hdfs://hacluster/hbase/extdata/hbaseRestore","dfs.client.read.shortcircuit.streams.cache.size":"512","phoenix.mutate.batchSize":"1000","hbase.master.deletedfilescache.expiry.interval":"172800","hbase.master.maxclockskew":"30000","hbase.master.initializationmonitor.haltontimeout":"false","hbase.cluster.distributed":"true","hbase.fs.tmp.dir":"/tmp","hbase.az.expression":"REP:AZ1[0.34],AZ2[0.33],AZ3[0.33]","hbase.security.authorization":"true","hbase.regionserver.hlog.writer.impl":"org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter","master.rmi.registry.host":"127.0.0.1","hbase.server.useip.enabled":"true","hbase.assignment.maximum.attempts":"2147483647","hbase.http.filter.initializers":"com.huawei.hadoop.hbase.adapter.sso.FlowCtrlFilter,com.huawei.hadoop.hbase.adapter.sso.XSSFilterInitializer,com.huawei.hadoop.hbase.adapter.sso.InternalSpnegoFilter,com.huawei.hadoop.hbase.adapter.sso.CASClientFilter,com.huawei.hadoop.hbase.adapter.sso.SessionTimeOutFilterInitializer,com.huawei.hadoop.hbase.adapter.sso.LogoutFilterInitializer","dfs.client.read.shortcircuit":"true","hbase.az.health.status.threshold":"0.5","hbase.master.preload.tabledescriptors":"false","phoenix.coprocessor.maxMetaDataCacheTimeToLiveMs":"1800000","hbase.client.max.perserver.tasks":"5","hbase.master.procedure.threads":"20","hbase.rsgroup.fallback.enable":"true","hbase.regionserver.hotregion.handler.count":"66","HBASE_ZK_SSL_ENABLED":"false","hbase.metrics.rit.stuck.warning.threshold":"300000","hbase.huawei.restore.output.v2":"hdfs://hacluster/hbase/extdata/restore/output","hbase.client.keyvalue.maxsize":"10485760","hbase.client.scanner.caching":"100","hbase.rpc.timeout":"60000","hbase.status.multicast.publisher.bind.address.ip":"10.131.194.252","hbase.master.hfilecleaner.skip.trash.for.compacted.files":"true","hbase.zookeeper.property.clientPort":"2181","hbase.split.wal.zk.coordinated":"false","hbase.replication.bulkload.enabled":"false","hbase.master.balancer.rejection.queue.size":"250","hbase.az.grouploadbalancer.class":"org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer","hbase.server.thread.wakefrequency":"10000","hbase.master.executor.closeregion.threads":"25","hbase.master.executor.serverops.threads":"50","hbase.regionserver.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.oldwals.cleaner.thread.size":"5","hbase.metric.controller.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com","hbase.master.executor.openregion.threads":"25","hregion.hfile.skip.errors":"false","hbase.assignment.dispatch.wait.queue.max.size":"100","hbase.fileStream.cleaner.ttl.thread.wakefrequency":"86400","hbase.master.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.rsgroup.grouploadbalancer.class":"org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer","master.rmi.registry.ip":"127.0.0.1","hbase.ipc.client.specificThreadForWriting":"false","hbase.replication.sync.hindex.specification":"false","hbase.zookeeper.quorum":"10.131.195.229,10.131.194.252,10.131.195.79","hbase.metric.controller.ssl.enabled":"true","hbase.master.balancer.rejection.buffer.enabled":"true","hbase.client.write.buffer":"2097152","hbase.filestream.rootdir":"/hbaseFileStream","hbase.rpc.protection":"privacy","hbase.crypto.keyprovider.parameters.uri":"","hbase.regionserver.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM","hbase.rsgroup.ui.modify.wait.ms":"5000","hbase.security.authentication.spnego.admin.users":"hbase","hbase.coprocessor.user.enabled":"true","hbase.master.metafixer.max.merge.count":"64","hbase.ranger.enabled":"true","hbase.client.ipc.pool.size":"5","hbase.external.data.rootdir":"hdfs://hacluster/hbase/extdata","dfs.domain.socket.path":"/var/run/FusionInsight-HDFS/dn_socket","hbase.master.ui.readonly":"true","hbase.meta.replicas.use":"false","hbase.regionserver.metahandler.count":"200","phoenix.schema.isNamespaceMappingEnabled":"true","hbase.renegotiation.allowed":"false","hbase.master.loadbalancer.class":"org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer","hbase.master.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM","hbase.metric.report.provider":"org.apache.hadoop.hbase.regionserver.hotspot.HotspotMetricCollectorImpl","hbase.replication.sync.namespace.schema":"false","hbase.coprocessor.abortonerror":"true","hbase.az.health.monitor.chore.interval":"300000","hbase.priority.rsgroup.enabled":"false","hbase.basic.auth.enabled":"false","hbase.mirror.table.state.to.zookeeper":"false","hbase.gsi.index.column.encoding":"FAST_DIFF","ip.model":"IPV4","hbase.master.abort.on.load.deletedfilescache.failure":"true","hbase.master.wait.on.regionservers.timeout":"4500","hbase.superuser":"hbase,@supergroup,@zkclient,@System_administrator_186","hbase.master.logcleaner.ttl":"600000","hbase.regionserver.handler.count":"200","hbase.master.ipc.address":"mrs-adp-q02-node-master2femp.mrs-h69n.com","hbase.assignment.dispatch.wait.msec":"150","dfs.client.read.shortcircuit.streams.cache.expiry.ms":"100000","hbase.crypto.cipherprovider":"com.huawei.hadoop.hbase.io.crypto.HuaweiCipherProvider","phoenix.default.column.encoded.bytes.attrib":"0","hbase.replication.sync.table.schema":"true","hbase.rpc.server.impl":"org.apache.hadoop.hbase.ipc.NettyRpcServer","hbase.coprocessor.master.classes":"org.apache.hadoop.hbase.hindex.server.master.HIndexMasterCoprocessor,com.huawei.hadoop.hbase.backup.services.RecoveryCoprocessor,org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor,org.apache.hadoop.hbase.security.access.ReadOnlyClusterEnabler,org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint,org.apache.hadoop.hbase.hindex.global.master.GlobalIndexMasterCoprocessor","hbase.crypto.key.algorithm":"AES","hbase.master.balancer.decision.queue.size":"250","hbase.rpc.client.impl":"org.apache.hadoop.hbase.ipc.NettyRpcClient","hbase.regionserver.thread.hfilecleaner.throttle":"67108864","hbase.master.deletedfilescache.flusher.interval":"10800000","hbase.gsi.max.index.name.length":"18","hbase.metric.controller.port":"21328","hbase.assignment.dead.region.metric.chore.interval.msec":"120000","hbase.quota.enabled":"false","hbase.assignment.rit.chore.interval.msec":"60000","hbase.master.initializationmonitor.timeout":"3600000","hbase.replication.cluster.id":"","hbase.server.alias":"HBase","hbase.client.gsi.cache.enabled":"true","hbase.regionserver.hfilecleaner.small.queue.size":"10240","hbase.snapshot.master.timeout.millis":"300000","zookeeper.znode.parent":"/hbase","hbase.master.info.port":"16010","hbase.master.hfilecleaner.ttl":"300000","master.rmi.connector.port":"21306","hbase.client.primaryCallTimeout.multiget":"10000","hbase.coprocessor.user.region.classes":"","hbase.master.balancer.uselocality":"true","hbase.client.rpc.codec":"org.apache.hadoop.hbase.codec.KeyValueCodecWithTags","hbase.ipc.server.callqueue.scan.ratio":"0","hbase.coprocessor.master.enable.jmx":"true","hbase.meta.versions":"10","hbase.fileStream.cleaner.tmp.expire.time":"86400","zookeeper.huawei.backup.parent":"hwbackup/hbase","hbase.crypto.keyprovider":"org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider","hbase.client.pause":"100","hbase.master.info.bindAddress":"mrs-adp-q02-node-master2femp.mrs-h69n.com","master.rmi.registry.port":"21306","hadoop.http.authentication.logout":"https://10.131.194.94:20009/cas/logout?service=https://10.131.194.94:20026/HBase/HMaster/67/master-status","hbase.ipc.server.callqueue.read.ratio":"0","hbase.regionserver.hfilecleaner.large.thread.count":"5","hbase.gsi.index.column.compression":"SNAPPY","hbase.master.wait.on.regionservers.mintostart":"1","hbase.security.authentication.sensitive.enabled":"true","hbase.normalizer.period":"1800000","hbase.http.filter.referrerpolicy.value":"strict-origin-when-cross-origin","hbase.security.exec.permission.checks":"true","hbase.rootdir.perms":"1711","hbase.regionserver.hlog.reader.impl":"org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader","hbase.bulkload.write.tries.enabled":"false","hbase.master.mob.table.allowed":"false","hbase.unsafe.stream.capability.enforce":"true","hbase.fs.hot.cold.enabled":"false","hbase.regionserver.wal.encryption":"false","hbase.region.assignment.auto.recovery.enabled":"true","hbase.fileStream.cleaner.ttl.enable":"false"}],
        @String[SHUXIN_TEST2],
    ],




java.base/java.lang.Thread.run(Thread.java:829)

2026-01-15 15:49:45.947 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect - runSecuredErr:executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
java.lang.RuntimeException: executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:339)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:248)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.lambda$getSchemaTables$6(HbaseMrsPlugin.java:189)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:91)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.getSchemaTables(HbaseMrsPlugin.java:189)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseMrs$4(DataSourceConnectionHelper.java:176)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseMrs(DataSourceConnectionHelper.java:175)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.phoenix.exception.PhoenixIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.phoenix.util.ClientUtil.parseServerException(ClientUtil.java:69)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1806)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:2267)
        at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable(DelegateConnectionQueryServices.java:159)
        at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:3278)
        at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1064)
        at org.apache.phoenix.compile.CreateTableCompiler$CreateTableMutationPlan.execute(CreateTableCompiler.java:491)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:573)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:538)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:537)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:525)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:2244)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3657)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:272)
        at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:150)
        at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:229)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactoryNoPool.openConnection(DriverConnectionFactoryNoPool.java:50)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:260)
        ... 25 common frames omitted
Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.lambda$getTimerTask$0(ReadOnlyZKClient.java:285)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.run(HashedWheelTimer.java:713)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.ImmediateExecutor.execute(ImmediateExecutor.java:34)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:701)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:788)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:501)
        ... 1 common frames omitted
2026-01-15 15:49:45.947 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: Authentication: simple 操作失败: executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:95)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.getSchemaTables(HbaseMrsPlugin.java:189)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseMrs$4(DataSourceConnectionHelper.java:176)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseMrs(DataSourceConnectionHelper.java:175)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:339)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:248)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.lambda$getSchemaTables$6(HbaseMrsPlugin.java:189)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:91)
        ... 19 common frames omitted
Caused by: org.apache.phoenix.exception.PhoenixIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.phoenix.util.ClientUtil.parseServerException(ClientUtil.java:69)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1806)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:2267)
        at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable(DelegateConnectionQueryServices.java:159)
        at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:3278)
        at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1064)
        at org.apache.phoenix.compile.CreateTableCompiler$CreateTableMutationPlan.execute(CreateTableCompiler.java:491)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:573)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:538)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:537)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:525)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:2244)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3657)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:272)
        at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:150)
        at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:229)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactoryNoPool.openConnection(DriverConnectionFactoryNoPool.java:50)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:260)
        ... 25 common frames omitted
Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.lambda$getTimerTask$0(ReadOnlyZKClient.java:285)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.run(HashedWheelTimer.java:713)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.ImmediateExecutor.execute(ImmediateExecutor.java:34)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:701)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:788)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:501)
        ... 1 common frames omitted
2026-01-15 15:49:45.947 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseMrs(DataSourceConnectionHelper.java:175)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: Authentication: simple 操作失败: executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:95)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.getSchemaTables(HbaseMrsPlugin.java:189)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseMrs$4(DataSourceConnectionHelper.java:176)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 16 common frames omitted
Caused by: java.lang.RuntimeException: executeSql:[]Get Schema Error: Zookeeper LIST could not be completed in 30000 ms
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:339)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:248)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.lambda$getSchemaTables$6(HbaseMrsPlugin.java:189)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cybermeta.jdbc.plugins.HbaseMrsPlugin$HbaseMrsJdbcDialect.runSecured(HbaseMrsPlugin.java:91)
        ... 19 common frames omitted
Caused by: org.apache.phoenix.exception.PhoenixIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.phoenix.util.ClientUtil.parseServerException(ClientUtil.java:69)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1806)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:2267)
        at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable(DelegateConnectionQueryServices.java:159)
        at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:3278)
        at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1064)
        at org.apache.phoenix.compile.CreateTableCompiler$CreateTableMutationPlan.execute(CreateTableCompiler.java:491)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:573)
        at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:538)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:537)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:525)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:2244)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3657)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76)
        at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:3603)
        at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:272)
        at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:150)
        at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:229)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
        at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactoryNoPool.openConnection(DriverConnectionFactoryNoPool.java:50)
        at com.datacyber.cybermeta.jdbc.plugins.jdbc.AbstractJdbcDialect.getSchemaTables(AbstractJdbcDialect.java:260)
        ... 25 common frames omitted
Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: Zookeeper LIST could not be completed in 30000 ms
        at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.lambda$getTimerTask$0(ReadOnlyZKClient.java:285)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.run(HashedWheelTimer.java:713)
        at org.apache.hbase.thirdparty.io.netty.util.concurrent.ImmediateExecutor.execute(ImmediateExecutor.java:34)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:701)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$HashedWheelBucket.expireTimeouts(HashedWheelTimer.java:788)
        at org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:501)
        ... 1 common frames omitted
2026-01-15 15:49:45.947 ERROR [cyber-worker] [work-async-executo






























v




























2026-01-15 14:51:52.932 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-15 14:51:58.397 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:419)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Constructor threw an exception for org.apache.hadoop.hbase.ipc.RpcClientImpl
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:54)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:34)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:46)
        ... 37 common frames omitted
Caused by: java.lang.ExceptionInInitializerError: null
        at org.apache.hadoop.hbase.ipc.IPCUtil.<init>(IPCUtil.java:74)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.<init>(AbstractRpcClient.java:95)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1092)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1118)
        ... 42 common frames omitted
Caused by: java.lang.RuntimeException: Unexpected version format: 11.0.13
        at org.apache.hadoop.hbase.util.ClassSize.<clinit>(ClassSize.java:119)
        ... 46 common frames omitted
2026-01-15 14:51:58.397 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.utils.KerberosUtils - 执行异常
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 21 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:419)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Constructor threw an exception for org.apache.hadoop.hbase.ipc.RpcClientImpl
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:54)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:34)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:46)
        ... 37 common frames omitted
Caused by: java.lang.ExceptionInInitializerError: null
        at org.apache.hadoop.hbase.ipc.IPCUtil.<init>(IPCUtil.java:74)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.<init>(AbstractRpcClient.java:95)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1092)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1118)
        ... 42 common frames omitted
Caused by: java.lang.RuntimeException: Unexpected version format: 11.0.13
        at org.apache.hadoop.hbase.util.ClassSize.<clinit>(ClassSize.java:119)
        ... 46 common frames omitted
2026-01-15 14:51:58.398 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:105)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)






":0.0,"usedJvmMb":0.0,"usedMemMb":0.0}, response={"code":"200","data":true,"msg":"成功"}
2026-01-15 14:42:25.175 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-15 14:42:32.177 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-15 14:42:33.178 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-15 14:42:34.748 ERROR [cyber-worker] [work-async-executor-4] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:407)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Constructor threw an exception for org.apache.hadoop.hbase.ipc.RpcClientImpl
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:54)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:34)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:46)
        ... 37 common frames omitted
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ClassSize
        at org.apache.hadoop.hbase.ipc.IPCUtil.<init>(IPCUtil.java:74)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.<init>(AbstractRpcClient.java:95)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1092)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1118)
        ... 42 common frames omitted
2026-01-15 14:42:34.748 ERROR [cyber-worker] [work-async-executor-4] com.datacyber.cyberdata.common.utils.KerberosUtils - 执行异常
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 21 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:407)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Constructor threw an exception for org.apache.hadoop.hbase.ipc.RpcClientImpl
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:54)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:34)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:46)
        ... 37 common frames omitted
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ClassSize
        at org.apache.hadoop.hbase.ipc.IPCUtil.<init>(IPCUtil.java:74)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.<init>(AbstractRpcClient.java:95)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1092)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1118)
        ... 42 common frames omitted
2026-01-15 14:42:34.749 ERROR [cyber-worker] [work-async-executor-4] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:105)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-15 14:42:34.749 ERROR [cyber-worker] [work-async-executor-4] com.datacyber.cyberdata.worker.service.metadata.








2026-01-15 14:17:39.098 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-15 14:17:39.098 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - kerberosInfo enabled: true, principal: u_meta
2026-01-15 14:17:39.109 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.common.utils.KerberosUtils - Set JAAS config for ZooKeeper SASL: /home/datac/tmp/kerberos/jaas_d228607ca10f107299335db3c0957af0.conf
2026-01-15 14:17:40.001 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.scheduler.WorkerSelfRegistry - HealthCheckEnable=true
2026-01-15 14:17:40.030 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.api.service.impl.WorkerRpcServiceImpl - Heartbeat report ur

2026-01-15 14:18:32.620 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-15 14:18:36.622 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-15 14:18:39.313 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:407)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Constructor threw an exception for org.apache.hadoop.hbase.ipc.RpcClientImpl
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:54)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:34)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:46)
        ... 37 common frames omitted
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ClassSize
        at org.apache.hadoop.hbase.ipc.IPCUtil.<init>(IPCUtil.java:74)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.<init>(AbstractRpcClient.java:95)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1092)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1118)
        ... 42 common frames omitted
2026-01-15 14:18:39.314 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.common.utils.KerberosUtils - 执行异常
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:82)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 21 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:407)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Constructor threw an exception for org.apache.hadoop.hbase.ipc.RpcClientImpl
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:54)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:34)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiate(ReflectionUtils.java:46)
        ... 37 common frames omitted
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ClassSize
        at org.apache.hadoop.hbase.ipc.IPCUtil.<init>(IPCUtil.java:74)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.<init>(AbstractRpcClient.java:95)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1092)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.<init>(RpcClientImpl.java:1118)
        ... 42 common frames omitted
2026-01-15 14:18:39.314 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:105)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-15 14:18:39.314 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - collect metadata has exception, recordId:63810com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常


























2026-01-15 00:08:20.000 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.scheduler.WorkerSelfRegistry - HealthCheckEnable=true
2026-01-15 00:08:20.032 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.api.service.impl.WorkerRpcServiceImpl - Heartbeat report url:http://coordinator-inner-dev:7500/coordinator/worker/heartbeat, request={"active":true,"availableThread":63,"cpuUsage":1.93,"createTime":"2026-01-15T00:04:54.472422000","env":"all","host":"10.131.9.130","jvmFreeMemory":4333196008,"jvmMaxMemory":6442450944,"maxCpu":3.0,"maxThread":64,"port":7800,"reservedJvmMb":7168.0,"resourceGroupNameEn":"default","sysFreeMemory":7857238016,"sysMaxMemory":12884901888,"type":"worker","updateTime":"2026-01-15T00:08:20.013154000","usedCpu":0.0,"usedJvmMb":0.0,"usedMemMb":0.0}, response={"code":"200","data":true,"msg":"成功"}
2026-01-15 00:08:20.134 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:407)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 33 common frames omitted
2026-01-15 00:08:20.135 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.utils.KerberosUtils - 执行异常
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 21 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:407)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 33 common frames omitted
2026-01-15 00:08:20.136 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:95)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-15 00:08:20.136 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - collect metadata has exception, recordId:63712com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
2026-01-15 00:08:20.322 INFO  [cyber-worker] [pool-3-thread-3] com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager - spark config: spark.sql.catalog.spark_catalog = org.apache.spark.sql.hudi.catalog.HoodieCatalog
2026-01-15 00:08:20.322 INFO  [cyber-worker] [pool-3-thread-3] com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager - spark config: spark.sql.extensions = org.apache.spark.sql.hive.SparkSessionExtension,io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.apache.spark.sql.hudi.HoodieSparkSessionExtension

















LF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2026-01-14 23:18:42.482 WARN  [cyber-worker] [work-async-executor-1] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/app/jdbc_driver_plugins/hbase-plugin-20260106021806.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
2026-01-14 23:18:43.443 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 33 common frames omitted
2026-01-14 23:18:43.444 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.utils.KerberosUtils - 执行异常
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 21 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 33 common frames omitted
2026-01-14 23:18:43.444 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:95)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-14 23:18:43.445 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.metada














2026-01-14 22:58:21 INFO  Worker: 10.131.11.232:7800 receives the metadata collection task2026-01-14 22:58:21 INFO  Metadata collection parameters information:2026-01-14 22:58:21 INFO  【taskId】:1921, 【recordId】:636992026-01-14 22:58:21 INFO  【datasource type】:HBASE2026-01-14 22:58:21 INFO  【datasource id】:20114257824027156492026-01-14 22:58:21 INFO  【datasource connection id】:20114257824740188182026-01-14 22:58:21 INFO  【datasource jdbc url】:null2026-01-14 22:58:21 INFO  【database name】:SHUXIN_TEST22026-01-14 22:58:21 INFO  【schema name】:SHUXIN_TEST22026-01-14 22:58:21 INFO  【use DBA Table】:false2026-01-14 22:58:23 ERROR An exception occurs in the collection task com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常	at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:98)	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(&lt;generated&gt;)	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)	at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)	at java.base/java.lang.Thread.run(Thread.java:829)2026-01-14 22:58:24 INFO  The collection task has been completed, A total of 0 tables have been collected.2026-01-14 22:58:24 INFO  Clean up the collection context information2026-01-14 22:58:24 INFO  END-EOF


WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
2026-01-14 23:03:59.332 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 33 common frames omitted
2026-01-14 23:03:59.333 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.utils.KerberosUtils - 执行异常
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 21 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 33 common frames omitted
2026-01-14 23:03:59.333 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:95)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-14 23:03:59.334 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - collect metadata has exception, recordId:63702com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常


























026-01-14 22:23:19.092 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-14 22:23:20.001 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.scheduler.WorkerSelfRegistry - HealthCheckEnable=true
2026-01-14 22:23:20.025 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.api.service.impl.WorkerRpcServiceImpl - Heartbeat report url:http://coordinator-inner-dev:7500/coordinator/worker/heartbeat, request={"active":true,"availableThread":63,"cpuUsage":0.54,"createTime":"2026-01-14T22:07:02.316441000","env":"all","host":"10.131.8.25","jvmFreeMemory":5622446048,"jvmMaxMemory":6442450944,"maxCpu":3.0,"maxThread":64,"port":7800,"reservedJvmMb":7168.0,"resourceGroupNameEn":"default","sysFreeMemory":7475478528,"sysMaxMemory":12884901888,"type":"worker","updateTime":"2026-01-14T22:23:20.009381000","usedCpu":0.0,"usedJvmMb":0.0,"usedMemMb":0.0}, response={"code":"200","data":true,"msg":"成功"}
2026-01-14 22:23:23.802 INFO  [cyber-worker] [pool-16-thread-1] com.datacyber.cyberdata.common.cache.RedisService - tryWaitingLock,lockKey:file_monitor_service:main_lock
2026-01-14 22:23:23.804 INFO  [cyber-worker] [pool-16-thread-1] com.datacyber.cyberdata.worker.FileMonitorService - ==fileScan==scanInterval:30
2026-01-14 22:23:24.277 ERROR [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Unable to find org.apache.hadoop.hbase.ipc.NettyRpcClient
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:36)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.ipc.NettyRpcClient
        at org.pf4j.PluginClassLoader.loadClass(PluginClassLoader.java:144)
        at java.base/java.lang.Class.forName0(Native Method)
        at java.base/java.lang.Class.forName(Class.java:315)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:32)
        ... 36 common frames omitted
2026-01-14 22:23:24.277 ERROR [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.common.utils.KerberosUtils - 执行异常
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$4(DataSourceConnectionHelper.java:160)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:72)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$null$3(DataSourceConnectionHelper.java:161)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 21 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 24 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 28 common frames omitted
Caused by: java.lang.UnsupportedOperationException: Unable to find org.apache.hadoop.hbase.ipc.NettyRpcClient
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:36)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:64)
        at org.apache.hadoop.hbase.ipc.RpcClientFactory.createClient(RpcClientFactory.java:48)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:638)
        ... 33 common frames omitted
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.ipc.NettyRpcClient
        at org.pf4j.PluginClassLoader.loadClass(PluginClassLoader.java:144)
        at java.base/java.lang.Class.forName0(Native Method)
        at java.base/java.lang.Class.forName(Class.java:315)
        at org.apache.hadoop.hbase.util.ReflectionUtils.instantiateWithCustomCtor(ReflectionUtils.java:32)
        ... 36 common frames omitted
2026-01-14 22:23:24.277 ERROR [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:95)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-14 22:23:24.278 ERROR [cyber-worker] [work-async-executor-3] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - collect metadata has exception, recordId:63697com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_086123b8d78bdc9a1f00229593d7dfb4.conf, login failed: 执行异常


























led":"true","hbase.fileStream.cleaner.ttl.enable":"false"}
2026-01-14 21:48:25.756 INFO  [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-14 21:48:25.756 INFO  [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - kerberosInfo enabled: true, principal: u_meta
2026-01-14 21:48:25.791 ERROR [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.common.utils.KerberosUtils - failure to login: for principal: u_meta from keytab /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_63e6b810ec137d6350d723c1c86c1ec7.keytab javax.security.auth.login.LoginException: Clients credentials have been revoked (18) - LOCKED_OUT
org.apache.hadoop.security.KerberosAuthException: failure to login: for principal: u_meta from keytab /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_63e6b810ec137d6350d723c1c86c1ec7.keytab javax.security.auth.login.LoginException: Clients credentials have been revoked (18) - LOCKED_OUT
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1986)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1361)
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:68)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: javax.security.auth.login.LoginException: Clients credentials have been revoked (18) - LOCKED_OUT
        at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:784)
        at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:592)
        at java.base/javax.security.auth.login.LoginContext.invoke(LoginContext.java:726)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:665)
        at java.base/javax.security.auth.login.LoginContext$4.run(LoginContext.java:663)
        at java.base/java.security.AccessController.doPrivileged(Native Method)
        at java.base/javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:663)
        at java.base/javax.security.auth.login.LoginContext.login(LoginContext.java:574)
        at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:2065)
        at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1975)
        ... 18 common frames omitted
Caused by: sun.security.krb5.KrbException: Clients credentials have been revoked (18) - LOCKED_OUT
        at java.security.jgss/sun.security.krb5.KrbAsRep.<init>(KrbAsRep.java:82)
        at java.security.jgss/sun.security.krb5.KrbAsReqBuilder.send(KrbAsReqBuilder.java:345)
        at java.security.jgss/sun.security.krb5.KrbAsReqBuilder.action(KrbAsReqBuilder.java:498)
        at jdk.security.auth/com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:756)
        ... 27 common frames omitted
Caused by: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)
        at java.security.jgss/sun.security.krb5.internal.KDCRep.init(KDCRep.java:140)
        at java.security.jgss/sun.security.krb5.internal.ASRep.init(ASRep.java:64)
        at java.security.jgss/sun.security.krb5.internal.ASRep.<init>(ASRep.java:59)
        at java.security.jgss/sun.security.krb5.KrbAsRep.<init>(KrbAsRep.java:60)
        ... 30 common frames omitted
2026-01-14 21:48:25.792 ERROR [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
java.lang.RuntimeException: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_63e6b810ec137d6350d723c1c86c1ec7.conf, login failed: failure to login: for principal: u_meta from keytab /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_63e6b810ec137d6350d723c1c86c1ec7.keytab javax.security.auth.login.LoginException: Clients credentials have been revoked (18) - LOCKED_OUT
        at com.datacyber.cyberdata.common.utils.KerberosUtils.runSecured(KerberosUtils.java:95)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:159)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-14 21:48:25.792 ERROR [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - collect metadata has exception, recordId:63688com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: authentication: kerberos /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_63e6b810ec137d6350d723c1c86c1ec7.conf, login failed: failure to login: for principal: u_meta from keytab /home/datac/tmp/kerberos/kerberos_b74a31064fbd7f3773d2f9c5bc0ca266_63e6b810ec137d6350d723c1c86c1ec7.keytab javax.security.auth.login.LoginException: Clients credentials have been revoked (18) - LOCKED_OUT





W, dataKind=null, engine=null, options=null, rows=0, dataLength=0, createTime=null, updateTime=null, columns=null, primaryKeys=[], partitionKeys=[], bucketKeys=[], indexInfos=[], constraintInfos=[], sharkKeyList=null, properties=null)]
2026-01-14 21:39:16.167 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-14 21:39:19.169 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-14 21:39:20.000 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.scheduler.WorkerSelfRegistry - HealthCheckEnable=true
2026-01-14 21:39:20.005 INFO  [cyber-worker] [http-nio-7800-exec-9] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - update system inner kerberos conf: datasourceConnection={"catalogName":"mrs_hbase_catalog","collectSwitch":0,"connectivityInfo":"{\"connectivityStatus\":true,\"errorMessage\":\"\",\"resourceGroupId\":1,\"testTime\":\"2026-01-14T21:14:16.258\"}","connectivityStatus":1,"createEngineSwitch":0,"createTime":"2026-01-14T21:10:51","createUser":1,"datasourceId":2011425782402715649,"detail":"{}","env":2,"id":2011425782474018818,"loginType":4,"state":1,"storageClusterId":1992956917615718402,"testTime":"2026-01-14T21:14:16","updateTime":"2026-01-14T21:10:51"}, kerberosConfName=u_meta
2026-01-14 21:39:20.007 INFO  [cyber-worker] [http-nio-7800-exec-9] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - select system inner kerberos conf: kerberosConf=null, kerberosConfName=u_meta
2026-01-14 21:39:20.007 INFO  [cyber-worker] [http-nio-7800-exec-9] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - datasource connection detail updated, datasourceConnectionId:2011425782474018818, loginType:4, hasKerberosInfo:false
2026-01-14 21:39:20.007 INFO  [cyber-worker] [http-nio-7800-exec-9] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - Received execute collect metadata request, recordId:63687, datasourceConnectionId:2011425782402715649, databaseName:SHUXIN_TEST2, schemaName:SHUXIN_TEST2, tableNames:null, tempTableNames:[]
2026-01-14 21:39:20.008 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService - start to collect metadata, req:MetadataCollectorReq(taskId=1921, recordId=63687, databaseName=SHUXIN_TEST2, schemaName=SHUXIN_TEST2, tableNames=null, collectionMode=0, filterMode=0, temporaryTableNames=[], regex=, datasourceConnection=MetadataCollectorReq.DatasourceConnection(id=2011425782474018818, datasourceId=2011425782402715649, type=HBASE, loginType=4, detail={}, compatibleMode=null, mode=0, kerberosInfo=null, version=null), async=true, collectBatchSize=2000, selectBatchSize=500, commitBatchSize=500, useDBATable=false)
2026-01-14 21:39:20.012 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - update system inner kerberos conf: datasourceConnection={"catalogName":"mrs_hbase_catalog","collectSwitch":0,"connectivityInfo":"{\"connectivityStatus\":true,\"errorMessage\":\"\",\"resourceGroupId\":1,\"testTime\":\"2026-01-14T21:14:16.258\"}","connectivityStatus":1,"createEngineSwitch":0,"createTime":"2026-01-14T21:10:51","createUser":1,"datasourceId":2011425782402715649,"detail":"{}","env":2,"id":2011425782474018818,"loginType":4,"state":1,"storageClusterId":1992956917615718402,"testTime":"2026-01-14T21:14:16","updateTime":"2026-01-14T21:10:51"}, kerberosConfName=u_meta
2026-01-14 21:39:20.013 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - select system inner kerberos conf: kerberosConf=null, kerberosConfName=u_meta
2026-01-14 21:39:20.013 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - detail信息:{}
2026-01-14 21:39:20.017 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - cfgInfo:{"master.rmi.RMIClientSocketFactory.class":"com.huawei.bigdata.om.controller.api.extern.monitor.RmiClientLocalhostSocketFactory","phoenix.schema.mapSystemTablesToNamespace":"true","hbase.status.publish.period":"10000","hbase.master.loadbalance.bytable":"true","hbase.ipc.server.metacallqueue.read.ratio":"0.5","dfs.replication":"3","hbase.master.port":"16000","hbase.replication.sync.table.schema.on.delete":"false","hbase.hmaster.ip.lists":"mrs-adp-q02-node-master3qkoz.mrs-h69n.com,mrs-adp-q02-node-master2femp.mrs-h69n.com","hbase.client.primaryCallTimeout.get":"10000","hbase.server":"HBase","hbase.dfs.client.read.shortcircuit.buffer.size":"131072","hbase.ipc.server.hotregion.max.callqueue.length":"330","hbase.fileStream.cleaner.tmp.thread.wakefrequency":"3600","hbase.regionserver.wal.codec":"org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec","hbase.client.zookeeper.property.clientPort":"2181","hbase.hmaster.hfilecleaner.trash.enabled":"true","hbase.client.scanner.timeout.period":"360000","hbase.large.object.threshold.max":"10485760","hbase.rootdir":"hdfs://hacluster/hbase","hbase.coprocessor.enabled":"true","hfile.format.version":"3","hbase.regionserver.hfilecleaner.large.queue.size":"10240","hbase.security.authentication":"kerberos","hbase.master.cleaner.interval":"60000","hbase.splitlog.manager.timeout":"600000","hbase.gsi.max.index.count.per.table":"5","hbase.snapshot.enabled":"true","zookeeper.session.timeout":"90000","phoenix.coprocessor.maxServerCacheTimeToLiveMs":"1800000","jmx.include.hosts":".*","hbase.quota.refresh.period":"300000","hbase.replication.sync.table.schema.on.create":"false","hbase.auth.key.update.interval":"86400000","hbase.security.authentication.spnego.admin.groups":"hbase,supergroup,System_administrator_186","hbase.master.balancer.stochastic.maxRunningTime":"300000","hbase.regionserver.hfilecleaner.small.thread.count":"5","hbase.crypto.master.key.name":"omm","hbase.huawei.restore.tmpdir":"hdfs://hacluster/hbase/extdata/restore/tmp","hbase.cleaner.scan.dir.concurrent.size":"0.5","hbase.ipc.server.callqueue.handler.factor":"0.1","hbase.fileStream.cleaner.ttl.expire.time":"604800","hbase.ssl.enabled":"true","hbase.metric.controller.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.master.namespace.init.timeout":"3600000","hbase.http.servlet.cookie.samesite":"Strict","hbase.crypto.keyprovider.parameters":"?encryptedtext=","hbase.master.truncate.fix.inconsistent":"true","hbase.crypto.wal.algorithm":"AES","hbase.assignment.retry.immediately.maximum.attempts":"3","hbase.huawei.backup.output":"/user/hbase/hbaseBackup/hbase","default.hbase.superuser":"hbase,@supergroup,@zkclient,@System_administrator_186","hbase.client.replicaCallTimeout.scan":"1000000","hbase.cold.rootdir":"/hbase","zookeeper.registry.async.get.timeout":"30000","hbase.hmaster.hfilecleaner.force.delete":"true","hbase.crypto.keyprovider.parameters.encryptedtext":"","hbase.util.ip.to.rack.determiner":"org.apache.hadoop.net.ScriptBasedMapping","hbase.config.crypt.class":"com.huawei.hadoop.datasight.security.FMHbaseCryptAdapter","hbase.data.rootdir":"/hbase","hbase.client.retries.number":"35","hbase.netty.worker.count":"0","hbase.master.balancer.decision.buffer.enabled":"true","hbase.huawei.restore.output":"hdfs://hacluster/hbase/extdata/hbaseRestore","dfs.client.read.shortcircuit.streams.cache.size":"512","phoenix.mutate.batchSize":"1000","hbase.master.deletedfilescache.expiry.interval":"172800","hbase.master.maxclockskew":"30000","hbase.master.initializationmonitor.haltontimeout":"false","hbase.cluster.distributed":"true","hbase.fs.tmp.dir":"/tmp","hbase.az.expression":"REP:AZ1[0.34],AZ2[0.33],AZ3[0.33]","hbase.security.authorization":"true","hbase.regionserver.hlog.writer.impl":"org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter","master.rmi.registry.host":"127.0.0.1","hbase.server.useip.enabled":"true","hbase.assignment.maximum.attempts":"2147483647","hbase.http.filter.initializers":"com.huawei.hadoop.hbase.adapter.sso.FlowCtrlFilter,com.huawei.hadoop.hbase.adapter.sso.XSSFilterInitializer,com.huawei.hadoop.hbase.adapter.sso.InternalSpnegoFilter,com.huawei.hadoop.hbase.adapter.sso.CASClientFilter,com.huawei.hadoop.hbase.adapter.sso.SessionTimeOutFilterInitializer,com.huawei.hadoop.hbase.adapter.sso.LogoutFilterInitializer","dfs.client.read.shortcircuit":"true","hbase.az.health.status.threshold":"0.5","hbase.master.preload.tabledescriptors":"false","phoenix.coprocessor.maxMetaDataCacheTimeToLiveMs":"1800000","hbase.client.max.perserver.tasks":"5","hbase.master.procedure.threads":"20","hbase.rsgroup.fallback.enable":"true","hbase.regionserver.hotregion.handler.count":"66","HBASE_ZK_SSL_ENABLED":"false","hbase.metrics.rit.stuck.warning.threshold":"300000","hbase.huawei.restore.output.v2":"hdfs://hacluster/hbase/extdata/restore/output","hbase.client.keyvalue.maxsize":"10485760","hbase.client.scanner.caching":"100","hbase.rpc.timeout":"60000","hbase.status.multicast.publisher.bind.address.ip":"10.131.194.252","hbase.master.hfilecleaner.skip.trash.for.compacted.files":"true","hbase.zookeeper.property.clientPort":"2181","hbase.split.wal.zk.coordinated":"false","hbase.replication.bulkload.enabled":"false","hbase.master.balancer.rejection.queue.size":"250","hbase.az.grouploadbalancer.class":"org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer","hbase.server.thread.wakefrequency":"10000","hbase.master.executor.closeregion.threads":"25","hbase.master.executor.serverops.threads":"50","hbase.regionserver.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.oldwals.cleaner.thread.size":"5","hbase.metric.controller.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com","hbase.master.executor.openregion.threads":"25","hregion.hfile.skip.errors":"false","hbase.assignment.dispatch.wait.queue.max.size":"100","hbase.fileStream.cleaner.ttl.thread.wakefrequency":"86400","hbase.master.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.rsgroup.grouploadbalancer.class":"org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer","master.rmi.registry.ip":"127.0.0.1","hbase.ipc.client.specificThreadForWriting":"false","hbase.replication.sync.hindex.specification":"false","hbase.zookeeper.quorum":"10.131.195.229,10.131.194.252,10.131.195.79","hbase.metric.controller.ssl.enabled":"true","hbase.master.balancer.rejection.buffer.enabled":"true","hbase.client.write.buffer":"2097152","hbase.filestream.rootdir":"/hbaseFileStream","hbase.rpc.protection":"privacy","hbase.crypto.keyprovider.parameters.uri":"","hbase.regionserver.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM","hbase.rsgroup.ui.modify.wait.ms":"5000","hbase.security.authentication.spnego.admin.users":"hbase","hbase.coprocessor.user.enabled":"true","hbase.master.metafixer.max.merge.count":"64","hbase.ranger.enabled":"true","hbase.client.ipc.pool.size":"5","hbase.external.data.rootdir":"hdfs://hacluster/hbase/extdata","dfs.domain.socket.path":"/var/run/FusionInsight-HDFS/dn_socket","hbase.master.ui.readonly":"true","hbase.meta.replicas.use":"false","hbase.regionserver.metahandler.count":"200","phoenix.schema.isNamespaceMappingEnabled":"true","hbase.renegotiation.allowed":"false","hbase.master.loadbalancer.class":"org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer","hbase.master.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM","hbase.metric.report.provider":"org.apache.hadoop.hbase.regionserver.hotspot.HotspotMetricCollectorImpl","hbase.replication.sync.namespace.schema":"false","hbase.coprocessor.abortonerror":"true","hbase.az.health.monitor.chore.interval":"300000","hbase.priority.rsgroup.enabled":"false","hbase.basic.auth.enabled":"false","hbase.mirror.table.state.to.zookeeper":"false","hbase.gsi.index.column.encoding":"FAST_DIFF","ip.model":"IPV4","hbase.master.abort.on.load.deletedfilescache.failure":"true","hbase.master.wait.on.regionservers.timeout":"4500","hbase.superuser":"hbase,@supergroup,@zkclient,@System_administrator_186","hbase.master.logcleaner.ttl":"600000","hbase.regionserver.handler.count":"200","hbase.master.ipc.address":"mrs-adp-q02-node-master2femp.mrs-h69n.com","hbase.assignment.dispatch.wait.msec":"150","dfs.client.read.shortcircuit.streams.cache.expiry.ms":"100000","hbase.crypto.cipherprovider":"com.huawei.hadoop.hbase.io.crypto.HuaweiCipherProvider","phoenix.default.column.encoded.bytes.attrib":"0","hbase.replication.sync.table.schema":"true","hbase.rpc.server.impl":"org.apache.hadoop.hbase.ipc.NettyRpcServer","hbase.coprocessor.master.classes":"org.apache.hadoop.hbase.hindex.server.master.HIndexMasterCoprocessor,com.huawei.hadoop.hbase.backup.services.RecoveryCoprocessor,org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor,org.apache.hadoop.hbase.security.access.ReadOnlyClusterEnabler,org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint,org.apache.hadoop.hbase.hindex.global.master.GlobalIndexMasterCoprocessor","hbase.crypto.key.algorithm":"AES","hbase.master.balancer.decision.queue.size":"250","hbase.rpc.client.impl":"org.apache.hadoop.hbase.ipc.NettyRpcClient","hbase.regionserver.thread.hfilecleaner.throttle":"67108864","hbase.master.deletedfilescache.flusher.interval":"10800000","hbase.gsi.max.index.name.length":"18","hbase.metric.controller.port":"21328","hbase.assignment.dead.region.metric.chore.interval.msec":"120000","hbase.quota.enabled":"false","hbase.assignment.rit.chore.interval.msec":"60000","hbase.master.initializationmonitor.timeout":"3600000","hbase.replication.cluster.id":"","hbase.server.alias":"HBase","hbase.client.gsi.cache.enabled":"true","hbase.regionserver.hfilecleaner.small.queue.size":"10240","hbase.snapshot.master.timeout.millis":"300000","zookeeper.znode.parent":"/hbase","hbase.master.info.port":"16010","hbase.master.hfilecleaner.ttl":"300000","master.rmi.connector.port":"21306","hbase.client.primaryCallTimeout.multiget":"10000","hbase.coprocessor.user.region.classes":"","hbase.master.balancer.uselocality":"true","hbase.client.rpc.codec":"org.apache.hadoop.hbase.codec.KeyValueCodecWithTags","hbase.ipc.server.callqueue.scan.ratio":"0","hbase.coprocessor.master.enable.jmx":"true","hbase.meta.versions":"10","hbase.fileStream.cleaner.tmp.expire.time":"86400","zookeeper.huawei.backup.parent":"hwbackup/hbase","hbase.crypto.keyprovider":"org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider","hbase.client.pause":"100","hbase.master.info.bindAddress":"mrs-adp-q02-node-master2femp.mrs-h69n.com","master.rmi.registry.port":"21306","hadoop.http.authentication.logout":"https://10.131.194.94:20009/cas/logout?service=https://10.131.194.94:20026/HBase/HMaster/67/master-status","hbase.ipc.server.callqueue.read.ratio":"0","hbase.regionserver.hfilecleaner.large.thread.count":"5","hbase.gsi.index.column.compression":"SNAPPY","hbase.master.wait.on.regionservers.mintostart":"1","hbase.security.authentication.sensitive.enabled":"true","hbase.normalizer.period":"1800000","hbase.http.filter.referrerpolicy.value":"strict-origin-when-cross-origin","hbase.security.exec.permission.checks":"true","hbase.rootdir.perms":"1711","hbase.regionserver.hlog.reader.impl":"org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader","hbase.bulkload.write.tries.enabled":"false","hbase.master.mob.table.allowed":"false","hbase.unsafe.stream.capability.enforce":"true","hbase.fs.hot.cold.enabled":"false","hbase.regionserver.wal.encryption":"false","hbase.region.assignment.auto.recovery.enabled":"true","hbase.fileStream.cleaner.ttl.enable":"false"}
2026-01-14 21:39:20.017 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-14 21:39:20.017 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - kerberosInfo enabled: false, principal: null
2026-01-14 21:39:20.028 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.api.service.impl.WorkerRpcServiceImpl - Heartbeat report url:http://coordinator-inner-dev:7500/coordinator/worker/heartbeat, request={"active":true,"availableThread":63,"cpuUsage":1.23,"createTime":"2026-01-14T21:31:32.540009000","env":"all","host":"10.131.14.61","jvmFreeMemory":5086649944,"jvmMaxMemory":6442450944,"maxCpu":3.0,"maxThread":64,"port":7800,"reservedJvmMb":7168.0,"resourceGroupNameEn":"default","sysFreeMemory":7637733376,"sysMaxMemory":12884901888,"type":"worker","updateTime":"2026-01-14T21:39:20.010411000","usedCpu":0.0,"usedJvmMb":0.0,"usedMemMb":0.0}, response={"code":"200","data":true,"msg":"成功"}















2026-01-14 21:34:42.721 INFO  [cyber-worker] [http-nio-7800-exec-9] com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2026-01-14 21:34:42.721 INFO  [cyber-worker] [http-nio-7800-exec-9] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - no cache,init jdbcDialect connectionInfo:2010685314769162242
2026-01-14 21:34:42.721 INFO  [cyber-worker] [http-nio-7800-exec-9] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - dialect map size:0
2026-01-14 21:34:42.746 INFO  [cyber-worker] [http-nio-7800-exec-9] com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactory - total:1, active:1, idle:0, awaiting:0,maxPoolSize:30
2026-01-14 21:34:42.806 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$5(DataSourceConnectionHelper.java:169)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:168)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 19 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 23 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 28 common frames omitted
2026-01-14 21:34:42.806 ERROR [cyber-worker] [work-async-executor-1] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbaseWithKerberos(DataSourceConnectionHelper.java:168)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:88)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbaseWithKerberos$5(DataSourceConnectionHelper.java:169)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 16 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 19 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 23 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 28 common frames omitted
2026-01-14 21:34:42.807 ERROR [cyber-worker] [work-async-executor-1] com.datacy

















detail信息:{}
2026-01-14 20:33:38.043 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - cfgInfo:{"master.rmi.RMIClientSocketFactory.class":"com.huawei.bigdata.om.controller.api.extern.monitor.RmiClientLocalhostSocketFactory","phoenix.schema.mapSystemTablesToNamespace":"true","hbase.status.publish.period":"10000","hbase.master.loadbalance.bytable":"true","hbase.ipc.server.metacallqueue.read.ratio":"0.5","dfs.replication":"3","hbase.master.port":"16000","hbase.replication.sync.table.schema.on.delete":"false","hbase.hmaster.ip.lists":"mrs-adp-q02-node-master3qkoz.mrs-h69n.com,mrs-adp-q02-node-master2femp.mrs-h69n.com","hbase.client.primaryCallTimeout.get":"10000","hbase.server":"HBase","hbase.dfs.client.read.shortcircuit.buffer.size":"131072","hbase.ipc.server.hotregion.max.callqueue.length":"330","hbase.fileStream.cleaner.tmp.thread.wakefrequency":"3600","hbase.regionserver.wal.codec":"org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec","hbase.client.zookeeper.property.clientPort":"2181","hbase.hmaster.hfilecleaner.trash.enabled":"true","hbase.client.scanner.timeout.period":"360000","hbase.large.object.threshold.max":"10485760","hbase.rootdir":"hdfs://hacluster/hbase","hbase.coprocessor.enabled":"true","hfile.format.version":"3","hbase.regionserver.hfilecleaner.large.queue.size":"10240","hbase.security.authentication":"kerberos","hbase.master.cleaner.interval":"60000","hbase.splitlog.manager.timeout":"600000","hbase.gsi.max.index.count.per.table":"5","hbase.snapshot.enabled":"true","zookeeper.session.timeout":"90000","phoenix.coprocessor.maxServerCacheTimeToLiveMs":"1800000","jmx.include.hosts":".*","hbase.quota.refresh.period":"300000","hbase.replication.sync.table.schema.on.create":"false","hbase.auth.key.update.interval":"86400000","hbase.security.authentication.spnego.admin.groups":"hbase,supergroup,System_administrator_186","hbase.master.balancer.stochastic.maxRunningTime":"300000","hbase.regionserver.hfilecleaner.small.thread.count":"5","hbase.crypto.master.key.name":"omm","hbase.huawei.restore.tmpdir":"hdfs://hacluster/hbase/extdata/restore/tmp","hbase.cleaner.scan.dir.concurrent.size":"0.5","hbase.ipc.server.callqueue.handler.factor":"0.1","hbase.fileStream.cleaner.ttl.expire.time":"604800","hbase.ssl.enabled":"true","hbase.metric.controller.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.master.namespace.init.timeout":"3600000","hbase.http.servlet.cookie.samesite":"Strict","hbase.crypto.keyprovider.parameters":"?encryptedtext=","hbase.master.truncate.fix.inconsistent":"true","hbase.crypto.wal.algorithm":"AES","hbase.assignment.retry.immediately.maximum.attempts":"3","hbase.huawei.backup.output":"/user/hbase/hbaseBackup/hbase","default.hbase.superuser":"hbase,@supergroup,@zkclient,@System_administrator_186","hbase.client.replicaCallTimeout.scan":"1000000","hbase.cold.rootdir":"/hbase","zookeeper.registry.async.get.timeout":"30000","hbase.hmaster.hfilecleaner.force.delete":"true","hbase.crypto.keyprovider.parameters.encryptedtext":"","hbase.util.ip.to.rack.determiner":"org.apache.hadoop.net.ScriptBasedMapping","hbase.config.crypt.class":"com.huawei.hadoop.datasight.security.FMHbaseCryptAdapter","hbase.data.rootdir":"/hbase","hbase.client.retries.number":"35","hbase.netty.worker.count":"0","hbase.master.balancer.decision.buffer.enabled":"true","hbase.huawei.restore.output":"hdfs://hacluster/hbase/extdata/hbaseRestore","dfs.client.read.shortcircuit.streams.cache.size":"512","phoenix.mutate.batchSize":"1000","hbase.master.deletedfilescache.expiry.interval":"172800","hbase.master.maxclockskew":"30000","hbase.master.initializationmonitor.haltontimeout":"false","hbase.cluster.distributed":"true","hbase.fs.tmp.dir":"/tmp","hbase.az.expression":"REP:AZ1[0.34],AZ2[0.33],AZ3[0.33]","hbase.security.authorization":"true","hbase.regionserver.hlog.writer.impl":"org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter","master.rmi.registry.host":"127.0.0.1","hbase.server.useip.enabled":"true","hbase.assignment.maximum.attempts":"2147483647","hbase.http.filter.initializers":"com.huawei.hadoop.hbase.adapter.sso.FlowCtrlFilter,com.huawei.hadoop.hbase.adapter.sso.XSSFilterInitializer,com.huawei.hadoop.hbase.adapter.sso.InternalSpnegoFilter,com.huawei.hadoop.hbase.adapter.sso.CASClientFilter,com.huawei.hadoop.hbase.adapter.sso.SessionTimeOutFilterInitializer,com.huawei.hadoop.hbase.adapter.sso.LogoutFilterInitializer","dfs.client.read.shortcircuit":"true","hbase.az.health.status.threshold":"0.5","hbase.master.preload.tabledescriptors":"false","phoenix.coprocessor.maxMetaDataCacheTimeToLiveMs":"1800000","hbase.client.max.perserver.tasks":"5","hbase.master.procedure.threads":"20","hbase.rsgroup.fallback.enable":"true","hbase.regionserver.hotregion.handler.count":"66","HBASE_ZK_SSL_ENABLED":"false","hbase.metrics.rit.stuck.warning.threshold":"300000","hbase.huawei.restore.output.v2":"hdfs://hacluster/hbase/extdata/restore/output","hbase.client.keyvalue.maxsize":"10485760","hbase.client.scanner.caching":"100","hbase.rpc.timeout":"60000","hbase.status.multicast.publisher.bind.address.ip":"10.131.194.252","hbase.master.hfilecleaner.skip.trash.for.compacted.files":"true","hbase.zookeeper.property.clientPort":"2181","hbase.split.wal.zk.coordinated":"false","hbase.replication.bulkload.enabled":"false","hbase.master.balancer.rejection.queue.size":"250","hbase.az.grouploadbalancer.class":"org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer","hbase.server.thread.wakefrequency":"10000","hbase.master.executor.closeregion.threads":"25","hbase.master.executor.serverops.threads":"50","hbase.regionserver.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.oldwals.cleaner.thread.size":"5","hbase.metric.controller.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com","hbase.master.executor.openregion.threads":"25","hregion.hfile.skip.errors":"false","hbase.assignment.dispatch.wait.queue.max.size":"100","hbase.fileStream.cleaner.ttl.thread.wakefrequency":"86400","hbase.master.keytab.file":"/opt/Bigdata/FusionInsight_HD_8.6.0/install/FusionInsight-HBase-2.6.1/keytabs/HBase/hbase.keytab","hbase.rsgroup.grouploadbalancer.class":"org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer","master.rmi.registry.ip":"127.0.0.1","hbase.ipc.client.specificThreadForWriting":"false","hbase.replication.sync.hindex.specification":"false","hbase.zookeeper.quorum":"10.131.195.229,10.131.194.252,10.131.195.79","hbase.metric.controller.ssl.enabled":"true","hbase.master.balancer.rejection.buffer.enabled":"true","hbase.client.write.buffer":"2097152","hbase.filestream.rootdir":"/hbaseFileStream","hbase.rpc.protection":"privacy","hbase.crypto.keyprovider.parameters.uri":"","hbase.regionserver.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM","hbase.rsgroup.ui.modify.wait.ms":"5000","hbase.security.authentication.spnego.admin.users":"hbase","hbase.coprocessor.user.enabled":"true","hbase.master.metafixer.max.merge.count":"64","hbase.ranger.enabled":"true","hbase.client.ipc.pool.size":"5","hbase.external.data.rootdir":"hdfs://hacluster/hbase/extdata","dfs.domain.socket.path":"/var/run/FusionInsight-HDFS/dn_socket","hbase.master.ui.readonly":"true","hbase.meta.replicas.use":"false","hbase.regionserver.metahandler.count":"200","phoenix.schema.isNamespaceMappingEnabled":"true","hbase.renegotiation.allowed":"false","hbase.master.loadbalancer.class":"org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer","hbase.master.kerberos.principal":"hbase/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM","hbase.metric.report.provider":"org.apache.hadoop.hbase.regionserver.hotspot.HotspotMetricCollectorImpl","hbase.replication.sync.namespace.schema":"false","hbase.coprocessor.abortonerror":"true","hbase.az.health.monitor.chore.interval":"300000","hbase.priority.rsgroup.enabled":"false","hbase.basic.auth.enabled":"false","hbase.mirror.table.state.to.zookeeper":"false","hbase.gsi.index.column.encoding":"FAST_DIFF","ip.model":"IPV4","hbase.master.abort.on.load.deletedfilescache.failure":"true","hbase.master.wait.on.regionservers.timeout":"4500","hbase.superuser":"hbase,@supergroup,@zkclient,@System_administrator_186","hbase.master.logcleaner.ttl":"600000","hbase.regionserver.handler.count":"200","hbase.master.ipc.address":"mrs-adp-q02-node-master2femp.mrs-h69n.com","hbase.assignment.dispatch.wait.msec":"150","dfs.client.read.shortcircuit.streams.cache.expiry.ms":"100000","hbase.crypto.cipherprovider":"com.huawei.hadoop.hbase.io.crypto.HuaweiCipherProvider","phoenix.default.column.encoded.bytes.attrib":"0","hbase.replication.sync.table.schema":"true","hbase.rpc.server.impl":"org.apache.hadoop.hbase.ipc.NettyRpcServer","hbase.coprocessor.master.classes":"org.apache.hadoop.hbase.hindex.server.master.HIndexMasterCoprocessor,com.huawei.hadoop.hbase.backup.services.RecoveryCoprocessor,org.apache.ranger.authorization.hbase.RangerAuthorizationCoprocessor,org.apache.hadoop.hbase.security.access.ReadOnlyClusterEnabler,org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint,org.apache.hadoop.hbase.hindex.global.master.GlobalIndexMasterCoprocessor","hbase.crypto.key.algorithm":"AES","hbase.master.balancer.decision.queue.size":"250","hbase.rpc.client.impl":"org.apache.hadoop.hbase.ipc.NettyRpcClient","hbase.regionserver.thread.hfilecleaner.throttle":"67108864","hbase.master.deletedfilescache.flusher.interval":"10800000","hbase.gsi.max.index.name.length":"18","hbase.metric.controller.port":"21328","hbase.assignment.dead.region.metric.chore.interval.msec":"120000","hbase.quota.enabled":"false","hbase.assignment.rit.chore.interval.msec":"60000","hbase.master.initializationmonitor.timeout":"3600000","hbase.replication.cluster.id":"","hbase.server.alias":"HBase","hbase.client.gsi.cache.enabled":"true","hbase.regionserver.hfilecleaner.small.queue.size":"10240","hbase.snapshot.master.timeout.millis":"300000","zookeeper.znode.parent":"/hbase","hbase.master.info.port":"16010","hbase.master.hfilecleaner.ttl":"300000","master.rmi.connector.port":"21306","hbase.client.primaryCallTimeout.multiget":"10000","hbase.coprocessor.user.region.classes":"","hbase.master.balancer.uselocality":"true","hbase.client.rpc.codec":"org.apache.hadoop.hbase.codec.KeyValueCodecWithTags","hbase.ipc.server.callqueue.scan.ratio":"0","hbase.coprocessor.master.enable.jmx":"true","hbase.meta.versions":"10","hbase.fileStream.cleaner.tmp.expire.time":"86400","zookeeper.huawei.backup.parent":"hwbackup/hbase","hbase.crypto.keyprovider":"org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider","hbase.client.pause":"100","hbase.master.info.bindAddress":"mrs-adp-q02-node-master2femp.mrs-h69n.com","master.rmi.registry.port":"21306","hadoop.http.authentication.logout":"https://10.131.194.94:20009/cas/logout?service=https://10.131.194.94:20026/HBase/HMaster/67/master-status","hbase.ipc.server.callqueue.read.ratio":"0","hbase.regionserver.hfilecleaner.large.thread.count":"5","hbase.gsi.index.column.compression":"SNAPPY","hbase.master.wait.on.regionservers.mintostart":"1","hbase.security.authentication.sensitive.enabled":"true","hbase.normalizer.period":"1800000","hbase.http.filter.referrerpolicy.value":"strict-origin-when-cross-origin","hbase.security.exec.permission.checks":"true","hbase.rootdir.perms":"1711","hbase.regionserver.hlog.reader.impl":"org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader","hbase.bulkload.write.tries.enabled":"false","hbase.master.mob.table.allowed":"false","hbase.unsafe.stream.capability.enforce":"true","hbase.fs.hot.cold.enabled":"false","hbase.regionserver.wal.encryption":"false","hbase.region.assignment.auto.recovery.enabled":"true","hbase.fileStream.cleaner.ttl.enable":"false"}
2026-01-14 20:33:38.043 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-14 20:33:38.142 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常





ed":"true","hbase.fileStream.cleaner.ttl.enable":"false"}
2026-01-14 20:33:38.043 INFO  [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-14 20:33:38.142 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbase$2(DataSourceConnectionHelper.java:144)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbase(DataSourceConnectionHelper.java:143)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:76)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 19 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 23 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)
        ... 28 common frames omitted
2026-01-14 20:33:38.143 ERROR [cyber-worker] [work-async-executor-2] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbase(DataSourceConnectionHelper.java:143)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:76)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbase$2(DataSourceConnectionHelper.java:144)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 16 common frames omitted
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:240)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:390)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 19 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
        ... 23 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:91)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:221)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:541)
        at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:880)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:636)














Affect(class count: 1 , method count: 1) cost in 233 ms, listenerId: 2
method=com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.extractHBaseCfgInfo location=AtExit
ts=2026-01-14 20:14:24.544; [cost=1.393698ms] result=@ArrayList[
    @Object[][
        @DatasourceConnection[
            serialVersionUID=@Long[1],
            id=@Long[2009903889270194178],
            datasourceId=@Long[2009903889010147330],
            env=@Integer[2],
            state=@Integer[1],
            collectSwitch=@Integer[0],
            createEngineSwitch=@Integer[0],
            loginType=@Integer[4],
            host=null,
            port=null,
            dbName=null,
            detail=@String[{}],
            storageClusterId=@Long[1992956917615718402],
            serverlessClusterId=null,
            kerberosId=null,
            sslTruststore=null,
            sslKeystore=null,
            iamRole=null,
            connectivityInfo=@String[{"connectivityStatus":true,"errorMessage":"","resourceGroupId":1,"testTime":"2026-01-14T16:27:09.990"}],
            catalogName=@String[hbase_test],
            createUser=@Long[1],
            createTime=@LocalDateTime[
                MIN=@LocalDateTime[-999999999-01-01T00:00],
                MAX=@LocalDateTime[+999999999-12-31T23:59:59.999999999],
                serialVersionUID=@Long[6207766400415563566],
                date=@LocalDate[2026-01-10],
                time=@LocalTime[16:23:24],
            ],
            updateUser=null,
            updateTime=@LocalDateTime[
                MIN=@LocalDateTime[-999999999-01-01T00:00],
                MAX=@LocalDateTime[+999999999-12-31T23:59:59.999999999],
                serialVersionUID=@Long[6207766400415563566],
                date=@LocalDate[2026-01-10],
                time=@LocalTime[16:23:24],
            ],
            compatibleMode=null,
            kerberosInfo=null,
            connectivityStatus=@Integer[1],
            testTime=@LocalDateTime[
                MIN=@LocalDateTime[-999999999-01-01T00:00],
                MAX=@LocalDateTime[+999999999-12-31T23:59:59.999999999],
                serialVersionUID=@Long[6207766400415563566],
                date=@LocalDate[2026-01-14],
                time=@LocalTime[16:27:10],
            ],
        ],
    ],
    null,
    null,
]









2026-01-14 20:11:05.615 INFO  [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - detail信息:{}
2026-01-14 20:11:05.615 INFO  [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - cfgInfo:null
2026-01-14 20:11:05.615 INFO  [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表，namespace: SHUXIN_TEST2
2026-01-14 20:11:05.658 ERROR [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder - 执行异常
java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbase$2(DataSourceConnectionHelper.java:144)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbase(DataSourceConnectionHelper.java:143)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:72)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: json参数无hbase.zookeeper.quorum键值
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:388)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 19 common frames omitted
2026-01-14 20:11:05.658 ERROR [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService - 获取HBase表列表失败
com.datacyber.cyberdata.common.exception.CommonException: 执行异常
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:433)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.listTablesHbase(DataSourceConnectionHelper.java:143)
        at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:72)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)
        at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: 获取hbase schema下的tables列表失败
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:70)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.getSchemaTables(HbasePlugin.java:102)
        at com.datacyber.cyberdata.common.task.datasource.DataSourceConnectionHelper.lambda$listTablesHbase$2(DataSourceConnectionHelper.java:144)
        at com.datacyber.cyberdata.common.task.datasource.JdbcDialectPluginHolder.executeWithContextClassLoader(JdbcDialectPluginHolder.java:430)
        ... 16 common frames omitted
Caused by: java.lang.RuntimeException: json参数无hbase.zookeeper.quorum键值
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.hbaseConnect(HbasePlugin.java:388)
        at com.datacyber.cybermeta.jdbc.plugins.HbasePlugin$HbaseJdbcDialect.execute(HbasePlugin.java:66)
        ... 19 common frames omitted
2026-01-14 20:11:05.658 ERROR [cyber-worker] [work-async-executor-8] com.datacyber.cyberdata.worker.service.metadata.MetadataColl











2026-01-14 20:05:23 INFO  Worker: 10.131.9.72:7800 receives the metadata collection task2026-01-14 20:05:23 INFO  Metadata collection parameters information:2026-01-14 20:05:23 INFO  【taskId】:1913, 【recordId】:636752026-01-14 20:05:23 INFO  【datasource type】:HBASE2026-01-14 20:05:23 INFO  【datasource id】:20099038890101473302026-01-14 20:05:23 INFO  【datasource connection id】:20099038892701941782026-01-14 20:05:23 INFO  【datasource jdbc url】:null2026-01-14 20:05:23 INFO  【database name】:SHUXIN_TEST22026-01-14 20:05:23 INFO  【schema name】:SHUXIN_TEST22026-01-14 20:05:23 INFO  【use DBA Table】:false2026-01-14 20:05:23 ERROR An exception occurs in the collection task com.datacyber.metaserver.common.exception.MetadataException: 获取HBase表列表失败: 执行异常	at com.datacyber.cyberdata.worker.service.accessor.impl.HBaseAccessService.listMetaDataTables(HBaseAccessService.java:82)	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.collect(MetadataCollectorService.java:209)	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService$$FastClassBySpringCGLIB$$d292a704.invoke(&lt;generated&gt;)	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)	at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)	at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)	at java.base/java.lang.Thread.run(Thread.java:829)2026-01-14 20:05:23 INFO  The collection task has been completed, A total of 0 tables have been collected.2026-01-14 20:05:23 INFO  Clean up the collection context information2026-01-14 20:05:23 INFO  END-EOF












2026-01-13 16:23:21,336 | INFO  | [pool-39-thread-1] | Trying to connect to metastore with URI thrift://MRS-ADP-Q02-node-master3QKOz.mrs-h69n.com:9083 | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:777)
2026-01-13 16:23:21,337 | INFO  | [pool-39-thread-1] | HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection. | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:844)
2026-01-13 16:23:21,337 | INFO  | [pool-39-thread-1] | Whether to use hadoop rpc protection: true | org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.getMetaStoreSaslProperties(MetaStoreUtils.java:1147)
2026-01-13 16:23:21,346 | INFO  | [pool-39-thread-1] | Opened a connection to metastore, current connections: 3 | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:871)
2026-01-13 16:23:21,347 | INFO  | [pool-39-thread-1] | Connected to metastore. | org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:967)
2026-01-13 16:23:21,347 | INFO  | [pool-39-thread-1] | RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=p_SuperAdmin@950C233D_AB6E_4AD9_AB28_F2766F074340.COM (auth:KERBEROS) retries=1 delay=1 lifetime=0 | org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:99)
2026-01-13 16:23:21,881 | WARN  | [pool-39-thread-1] | Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf | org.apache.hudi.common.config.DFSPropertiesConfiguration.getConfPathFromEnv(DFSPropertiesConfiguration.java:235)
2026-01-13 16:23:21,912 | WARN  | [pool-39-thread-1] | Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file | org.apache.hudi.common.config.DFSPropertiesConfiguration.addPropsFromFile(DFSPropertiesConfiguration.java:158)
2026-01-13 16:23:23,572 | INFO  | [pool-39-thread-1] | It took 1108 ms to build plan. | org.apache.spark.sql.execution.QueryExecution.logInfo(Logging.scala:60)
2026-01-13 16:23:24,290 | INFO  | [pool-39-thread-1] | Code generated in 429.98129 ms | org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.logInfo(Logging.scala:60)
2026-01-13 16:23:24,492 | INFO  | [pool-39-thread-1] | Block broadcast_0 stored as values in memory (estimated size 777.7 KiB, free 433.6 MiB) | org.apache.spark.storage.memory.MemoryStore.logInfo(Logging.scala:60)
2026-01-13 16:23:24,961 | INFO  | [pool-39-thread-1] | Block broadcast_0_piece0 stored as bytes in memory (estimated size 91.4 KiB, free 433.6 MiB) | org.apache.spark.storage.memory.MemoryStore.logInfo(Logging.scala:60)
2026-01-13 16:23:24,965 | INFO  | [dispatcher-BlockManagerMaster] | Added broadcast_0_piece0 in memory on MRS-ADP-Q02-hdfs-groupMUwL0001.mrs-h69n.com:22787 (size: 91.4 KiB, free: 434.3 MiB) | org.apache.spark.storage.BlockManagerInfo.logInfo(Logging.scala:60)
2026-01-13 16:23:24,998 | INFO  | [pool-39-thread-1] | Created broadcast 0 from  | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:25,255 | WARN  | [pool-39-thread-1] | Request failed, Response code: 403; Request ID: 0000019BB673DD6981099C0EF15CDFC0; Request path: http://adp-cluster-dev.obs.cn-east-273.antacloud.com/data%2Fhive%2Ftest01.db%2Fb_foitem_copy | com.obs.services.internal.RestStorageService.invoke0(NativeMethodAccessorImpl.java:-2)
2026-01-13 16:23:25,257 | WARN  | [pool-39-thread-1] | Storage|1|HTTP+XML|getObjectMetadata||||2026-01-13 16:23:25|2026-01-13 16:23:25|||403| | com.obs.services.AbstractClient.invoke0(NativeMethodAccessorImpl.java:-2)
2026-01-13 16:23:25,266 | ERROR | [pool-39-thread-1] | sql execute error:SELECT * from test01.b_foitem_copy | com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:228)
2026-01-13 16:23:29,996 | ERROR | [Driver] | org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0] | com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:244)
java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_462]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242) [custom-spark-submit-project.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_462]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_462]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_462]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_462]
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740) [spark-yarn_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.exception.ObsException: com.obs.services.exception.ObsException: Error message:Request Error.OBS service Error Message. -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAAheaders{error-message:Access Denied,request-id:0000019BB673DD6981099C0EF15CDFC0,content-length:0,id-2:36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA,error-code:AccessDenied,x-reserved-indicator:396,date:Tue, 13 Jan 2026 08:23:25 GMT,}
	at com.obs.services.internal.utils.ServiceUtils.changeFromServiceException(ServiceUtils.java:579) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:408) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.internal.ServiceException: Request Error.
	at com.obs.services.internal.RestStorageService.createServiceException(RestStorageService.java:702) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.handleRequestErrorResponse(RestStorageService.java:526) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.tryRequest(RestStorageService.java:512) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRequest(RestStorageService.java:387) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRestHead(RestStorageService.java:980) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectImpl(ObsObjectBaseService.java:229) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectMetadataImpl(ObsObjectBaseService.java:401) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.access$2000(AbstractObjectClient.java:60) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:665) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:661) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:398) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
2026-01-13 16:23:30,000 | INFO  | [Driver] | SparkContext is stopping with exitCode 0. | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:30,030 | INFO  | [Driver] | Stopped Spark web UI at http://MRS-ADP-Q02-hdfs-groupMUwL0001.mrs-h69n.com:22803 | org.apache.spark.ui.SparkUI.logInfo(Logging.scala:60)
2026-01-13 16:23:30,060 | INFO  | [Driver] | Shutting down all executors | org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend.logInfo(Logging.scala:60)
2026-01-13 16:23:30,064 | INFO  | [dispatcher-CoarseGrainedScheduler] | Asking each executor to shut down | org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnDriverEndpoint.logInfo(Logging.scala:60)
2026-01-13 16:23:30,200 | INFO  | [dispatcher-event-loop-0] | MapOutputTrackerMasterEndpoint stopped! | org.apache.spark.MapOutputTrackerMasterEndpoint.logInfo(Logging.scala:60)
2026-01-13 16:23:30,223 | INFO  | [Driver] | MemoryStore cleared | org.apache.spark.storage.memory.MemoryStore.logInfo(Logging.scala:60)
2026-01-13 16:23:30,224 | INFO  | [Driver] | BlockManager stopped | org.apache.spark.storage.BlockManager.logInfo(Logging.scala:60)
2026-01-13 16:23:30,246 | INFO  | [Driver] | BlockManagerMaster stopped | org.apache.spark.storage.BlockManagerMaster.logInfo(Logging.scala:60)
2026-01-13 16:23:30,252 | INFO  | [dispatcher-event-loop-1] | OutputCommitCoordinator stopped! | org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint.logInfo(Logging.scala:60)
2026-01-13 16:23:30,273 | INFO  | [Driver] | Successfully stopped SparkContext | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:30,273 | INFO  | [Driver] | SparkContext is stopping with exitCode 0. | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
2026-01-13 16:23:30,273 | INFO  | [Driver] | SparkContext already stopped. | org.apache.spark.SparkContext.logInfo(Logging.scala:60)
All resources have been released.
2026-01-13 16:23:30,275 | ERROR | [Driver] | User class threw exception:  | org.apache.spark.deploy.yarn.ApplicationMaster.logError(Logging.scala:97)
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:248) ~[custom-spark-submit-project.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_462]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_462]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_462]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_462]
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740) [spark-yarn_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_462]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242) ~[custom-spark-submit-project.jar:?]
	... 5 more
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.exception.ObsException: com.obs.services.exception.ObsException: Error message:Request Error.OBS service Error Message. -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAAheaders{error-message:Access Denied,request-id:0000019BB673DD6981099C0EF15CDFC0,content-length:0,id-2:36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA,error-code:AccessDenied,x-reserved-indicator:396,date:Tue, 13 Jan 2026 08:23:25 GMT,}
	at com.obs.services.internal.utils.ServiceUtils.changeFromServiceException(ServiceUtils.java:579) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:408) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
Caused by: com.obs.services.internal.ServiceException: Request Error.
	at com.obs.services.internal.RestStorageService.createServiceException(RestStorageService.java:702) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.handleRequestErrorResponse(RestStorageService.java:526) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.tryRequest(RestStorageService.java:512) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRequest(RestStorageService.java:387) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.RestStorageService.performRestHead(RestStorageService.java:980) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectImpl(ObsObjectBaseService.java:229) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectMetadataImpl(ObsObjectBaseService.java:401) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.access$2000(AbstractObjectClient.java:60) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:665) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:661) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:398) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921) ~[hadoop-huaweicloud-3.1.1-hw-54.6.6.jar:?]
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236) ~[hadoop-common-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342) ~[hadoop-mapreduce-client-core-3.3.1-h0.cbu.mrs.360.r9.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.19.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290) ~[spark-core_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315) ~[spark-sql_2.12-3.5.6-h0.cbu.mrs.360.r9.jar:3.5.6-h0.cbu.mrs.360.r9]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102) ~[custom-spark-submit-project.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_462]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_462]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_462]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_462]
2026-01-13 16:23:30,293 | INFO  | [Driver] | Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740)
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242)
	... 5 more
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691)
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564)
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527)
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936)
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942)
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145)
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at com.datacyber.cyberdata.spark.CustomSqlHandler.lambda$main$0(CustomSqlHandler.java:102)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.obs.services.exception.ObsException: Error message:Request Error.OBS service Error Message. -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAAheaders{error-message:Access Denied,request-id:0000019BB673DD6981099C0EF15CDFC0,content-length:0,id-2:36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA,error-code:AccessDenied,x-reserved-indicator:396,date:Tue, 13 Jan 2026 08:23:25 GMT,}
	at com.obs.services.internal.utils.ServiceUtils.changeFromServiceException(ServiceUtils.java:579)
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:408)
	at com.obs.services.AbstractObjectClient.getObjectMetadata(AbstractObjectClient.java:660)
	at com.obs.services.AbstractPFSClient.getAttribute(AbstractPFSClient.java:164)
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:546)
	... 64 more
Caused by: com.obs.services.internal.ServiceException: Request Error. HEAD 'http://adp-cluster-dev.obs.cn-east-273.antacloud.com/data%2Fhive%2Ftest01.db%2Fb_foitem_copy' on Host 'adp-cluster-dev.obs.cn-east-273.antacloud.com' @ 'Tue, 13 Jan 2026 08:23:25 GMT' -- ResponseCode: 403, ResponseStatus: Forbidden, RequestId: 0000019BB673DD6981099C0EF15CDFC0, HostId: 36AAAQAAEAABAAAQAAEAABAAAQAAEAABAAAaI=AAAAAAAAAAAAAAAAAAAAAAAAAA
	at com.obs.services.internal.RestStorageService.createServiceException(RestStorageService.java:702)
	at com.obs.services.internal.RestStorageService.handleRequestErrorResponse(RestStorageService.java:526)
	at com.obs.services.internal.RestStorageService.tryRequest(RestStorageService.java:512)
	at com.obs.services.internal.RestStorageService.performRequest(RestStorageService.java:387)
	at com.obs.services.internal.RestStorageService.performRestHead(RestStorageService.java:980)
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectImpl(ObsObjectBaseService.java:229)
	at com.obs.services.internal.service.ObsObjectBaseService.getObjectMetadataImpl(ObsObjectBaseService.java:401)
	at com.obs.services.AbstractObjectClient.access$2000(AbstractObjectClient.java:60)
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:665)
	at com.obs.services.AbstractObjectClient$14.action(AbstractObjectClient.java:661)
	at com.obs.services.AbstractClient.doActionWithResult(AbstractClient.java:398)
	... 67 more
) | org.apache.spark.deploy.yarn.ApplicationMaster.logInfo(Logging.scala:60)
2026-01-13 16:23:30,313 | INFO  | [shutdown-hook-0] | Deleting staging directory hdfs://hacluster/user/p_SuperAdmin/.sparkStaging/application_1768094372387_0569 | org.apache.spark.deploy.yarn.ApplicationMaster.logInfo(Logging.scala:60)
2026-01-13 16:23:30,334 | INFO  | [shutdown-hook-0] | Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:248)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:740)
Caused by: java.util.concurrent.ExecutionException: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at com.datacyber.cyberdata.spark.CustomSqlHandler.main(CustomSqlHandler.java:242)
	... 5 more
Caused by: org.apache.hadoop.security.AccessControlException: getFileStatus on obs://adp-cluster-dev/data/hive/test01.db/b_foitem_copy: ResponseCode[403],ErrorCode[AccessDenied],ErrorMessage[null
Request Error.],RequestId[0000019BB673DD6981099C0EF15CDFC0]
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:422)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.translateException(OBSCommonUtils.java:691)
	at org.apache.hadoop.fs.obs.OBSPosixBucketUtils.innerFsGetObjectStatus(OBSPosixBucketUtils.java:564)
	at org.apache.hadoop.fs.obs.OBSFileSystem.innerGetFileStatus(OBSFileSystem.java:1948)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.lambda$getFileStatusWithRetry$8(OBSCommonUtils.java:1528)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:68)
	at org.apache.hadoop.fs.obs.OBSInvoker.retryByMaxTime(OBSInvoker.java:55)
	at org.apache.hadoop.fs.obs.OBSCommonUtils.getFileStatusWithRetry(OBSCommonUtils.java:1527)
	at org.apache.hadoop.fs.obs.OBSFileSystem.getFileStatus(OBSFileSystem.java:1921)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:122)
	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:361)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:212)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2236)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:253)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:342)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:501)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:563)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:140)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:218)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:119)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:936)
	at org.apache.spark.sql.SparkSession.$anonfun$withFuseMonitor$1(SparkSession.scala:942)
	at org.apache.spark.sql.defense.DefenseContext.withFuseMonitor(DefenseContext.scala:145)
	at org.apache.spark.sql.SparkSession.withFuseMonitor(SparkSession.scala:942)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:71)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)



















2026-01-13 13:42:59 INFO  Current task status: Running2026-01-13 13:42:59 INFO  Start execute SPARK_SQL on worker_host: 10.131.11.232:78002026-01-13 13:42:59 INFO  Current log dir: /opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/1768282979365.log2026-01-13 13:42:59 INFO  Task executor: com.datacyber.cyberdata.worker.service.SparkSqlExecutorService2026-01-13 13:42:59 INFO  Start executing the Spark failover process2026-01-13 13:42:59 INFO  The spark applicationId is empty, preparing to start a new spark application2026-01-13 13:42:59 INFO  The environment is initialized2026-01-13 13:42:59 INFO  prepare the spark submit script......2026-01-13 13:42:59 INFO  spark task submit on yarn with depoly-mode cluster: 1001 2 iam_spark_dev2026-01-13 13:42:59 INFO  hadoop default fs: hdfs://hacluster2026-01-13 13:42:59 INFO  sql: SELECT * from test01.b_foitem_copy;2026-01-13 13:42:59 INFO  spark command:/opt/module/spark-3.5.6-mrs/bin/spark-submit --class com.datacyber.cyberdata.spark.CustomSqlHandler --master yarn --deploy-mode cluster --name test_saprk --conf 'spark.executor.memory=2g' --conf 'spark.driver.memory=1g' --conf 'spark.driver.cores=1' --conf 'spark.executor.cores=1' --conf 'spark.executor.instances=1' --conf 'spark.yarn.queue=default' /opt/module/spark-3.5.6-mrs/jars/custom-spark-submit-project.jar U0VMRUNUICogZnJvbSB0ZXN0MDEuYl9mb2l0ZW1fY29weTs= http://10.131.0.85:30303/worker/writeLog /opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/results true  2026-01-13 13:42:59 INFO  spark-submit command : /bin/bash /home/datac/shell/2010950683592646657.sh2026-01-13 13:42:59 INFO  Successful prepare the spark submit script......2026-01-13 13:42:59 WARN  1001,2,iam_spark_dev, kerberos info: null2026-01-13 13:42:59 INFO  Temporary ENV Variables2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  {}2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  Full Command2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  /bin/bash /home/datac/shell/2010950683592646657.sh2026-01-13 13:42:59 INFO  -------------------------2026-01-13 13:42:59 INFO  --- Invoking Shell command line now ---2026-01-13 13:42:59 INFO  =================================================================Warning: Ignoring non-Spark config property: hoodie.schema.evolution.enableWarning: Ignoring non-Spark config property: hoodie.archive.delete.parallelismWarning: Ignoring non-Spark config property: hoodie.clean.asyncWarning: Ignoring non-Spark config property: hoodie.clean.trigger.strategyWarning: Ignoring non-Spark config property: hoodie.use.hive.write.styleWarning: Ignoring non-Spark config property: hoodie.keep.min.commitsWarning: Ignoring non-Spark config property: token.server.fs.obs.endpointWarning: Ignoring non-Spark config property: hoodie.datasource.write.keygenerator.classWarning: Ignoring non-Spark config property: hoodie.datasource.write.payload.classWarning: Ignoring non-Spark config property: hoodie.cleaner.policyWarning: Ignoring non-Spark config property: fs.obs.delegation.token.onlyWarning: Ignoring non-Spark config property: hoodie.cleaner.parallelismWarning: Ignoring non-Spark config property: hoodie.parquet.compression.codecWarning: Ignoring non-Spark config property: hoodie.datasource.hive_sync.enableWarning: Ignoring non-Spark config property: hoodie.bulkinsert.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.parquet.small.file.limitWarning: Ignoring non-Spark config property: hoodie.cleaner.commits.retainedWarning: Ignoring non-Spark config property: hoodie.compact.inline.max.delta.commitsWarning: Ignoring non-Spark config property: hoodie.delete.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.archive.hours.retainedWarning: Ignoring non-Spark config property: hoodie.archive.policyWarning: Ignoring non-Spark config property: hoodie.upsert.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.clean.automaticWarning: Ignoring non-Spark config property: fs.obs.delegation.token.providers.ccWarning: Ignoring non-Spark config property: hoodie.compact.inlineWarning: Ignoring non-Spark config property: hoodie.insert.shuffle.parallelismWarning: Ignoring non-Spark config property: hoodie.datasource.write.hive_style_partitioningWarning: Ignoring non-Spark config property: hoodie.datasource.hive_sync.modeWarning: Ignoring non-Spark config property: hoodie.archive.asyncWarning: Ignoring non-Spark config property: hoodie.upgrade.enableWarning: Ignoring non-Spark config property: hoodie.cleaner.hours.retainedWarning: Ignoring non-Spark config property: hoodie.datasource.hive_sync.support_timestampWarning: Ignoring non-Spark config property: hoodie.rollback.parallelismWarning: Ignoring non-Spark config property: hoodie.archive.automaticWarning: Ignoring non-Spark config property: hoodie.schedule.compact.only.inlineWarning: Ignoring non-Spark config property: hoodie.keep.max.commitsWarning: Ignoring non-Spark config property: hoodie.run.compact.only.inline26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.rpc.numRetries' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.yarn.kerberos.relogin.period' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.relogin.period' instead.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.executor.plugins' has been deprecated as of Spark 3.0.0 and may be removed in the future. Feature replaced with new plugin API. See Monitoring documentation.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.reducer.maxReqSizeShuffleToMem' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.26/01/13 13:44:15 WARN SparkConf: The configuration key 'spark.rpc.retry.wait' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.rpc.numRetries' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.yarn.kerberos.relogin.period' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.relogin.period' instead.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.executor.plugins' has been deprecated as of Spark 3.0.0 and may be removed in the future. Feature replaced with new plugin API. See Monitoring documentation.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.reducer.maxReqSizeShuffleToMem' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.network.maxRemoteBlockSizeFetchToMem' instead.26/01/13 13:44:16 WARN SparkConf: The configuration key 'spark.rpc.retry.wait' has been deprecated as of Spark 2.2.0 and may be removed in the future. Not used anymore26/01/13 13:44:18 INFO AbstractService: Service org.apache.hadoop.yarn.client.api.impl.YarnClientImpl failed in state STARTEDjava.lang.IllegalArgumentException: Can't get Kerberos realm	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:78)	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:322)	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:307)	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:582)	at org.apache.hadoop.yarn.client.RMProxy.&lt;init&gt;(RMProxy.java:72)	at org.apache.hadoop.yarn.client.ClientRMProxy.&lt;init&gt;(ClientRMProxy.java:66)	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:79)	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:261)	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:202)	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:213)	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1403)	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1871)	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1076)	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:201)	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:224)	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:93)	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1167)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1176)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalArgumentException: KrbException: Cannot locate default realm	at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.&lt;init&gt;(KerberosPrincipal.java:179)	at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:128)	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:76)	... 18 moreException in thread "main" java.lang.IllegalArgumentException: Can't get Kerberos realm	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:78)	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:322)	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:307)	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:582)	at org.apache.hadoop.yarn.client.RMProxy.&lt;init&gt;(RMProxy.java:72)	at org.apache.hadoop.yarn.client.ClientRMProxy.&lt;init&gt;(ClientRMProxy.java:66)	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:79)	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:261)	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:202)	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:213)	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1403)	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1871)	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1076)	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:201)	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:224)	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:93)	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1167)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1176)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalArgumentException: KrbException: Cannot locate default realm	at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.&lt;init&gt;(KerberosPrincipal.java:179)	at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:128)	at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:76)	... 18 more26/01/13 13:44:18 INFO ShutdownHookManager: Shutdown hook called26/01/13 13:44:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-8cdb8aa8-b6dc-49f0-b17a-d365ca6a58d6


2026-01-13 13:42:59.482 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - -----read
2026-01-13 13:42:59.482 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - accessLogs size:0
2026-01-13 13:42:59.482 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - getParseInfoByMqDtoOver:[]
2026-01-13 13:42:59.483 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - 1 spark: taskDispatchMqDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010950683592646657, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 13:42:59 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/1768282979365.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 13:42:59.483 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: 执行executeTask线程名称:SPARK_SQL-TaskExecutor-2010950683592646657
2026-01-13 13:42:59.483 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - 2 spark: currentDispatchDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010950683592646657, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 13:42:59 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/1768282979365.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010950683592646657/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 13:42:59.483 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover获取到的appId为空
2026-01-13 13:42:59.483 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover结果:false
2026-01-13 13:42:59.486 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.common.utils.TaskVariablesUtils - functionValueMap2:{}
2026-01-13 13:42:59.486 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.common.utils.TaskVariablesUtils - replaceContent:SELECT * from test01.b_foitem_copy;
2026-01-13 13:42:59.495 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - 未找到引擎集群: clusterName=iam_spark_dev, tenantId=1001
2026-01-13 13:42:59.497 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkSqlExecutorService - federated datasource info: []
2026-01-13 13:42:59.497 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.service.SparkSqlExecutorService - federated query status:false
2026-01-13 13:42:59.499 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.dao.core.util.KerberosPersonalUtils - 未找到引擎集群: clusterName=iam_spark_dev, tenantId=1001
2026-01-13 13:42:59.521 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010950683592646657] com.datacyber.cyberdata.worker.utils.ProcessUtil - processClass name:java.lang.ProcessImpl
2026-01-13 13:43:00.000 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.scheduler.WorkerSelfRegistry - HealthCheckEnable=true
2026-01-13 13:43:00.023 INFO  [cyber-worker] [scheduling-1] com.datacyber.cyberdata.worker.api.service.impl.WorkerRpcServiceImpl - Heartbeat report url:http://coordinator-inner-dev:7500/coordinator/worker/heartbeat, request={"active":true,"availableThread":62,"cpuUsage":0.21,"createTime":"2026-01-13T11:17:37.051910000","env":"all","host":"10.131.11.232","jvmFreeMemory":2208031312,"jvmMaxMemory":6442450944,"maxCpu":3.0,"maxThread":64,"port":7800,"reservedJvmMb":7168.0,"resourceGroupNameEn":"default","sysFreeMemory":7454822400,"sysMaxMemory":12884901888,"type":"worker","updateTime":"2026-01-13T13:43:00.006677000","usedCpu":1.0,"usedJvmMb":2.0,"usedMemMb":1000.0}, response={"code":"200","data":true,"msg":"成功"}
2026-01-13 13:43:01.496 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描...
2026-01-13 13:43:04.534 INFO  [cyber-worker] [pool-17-thread-1] com.datacyber.cyberdata.worker.HdfsTempFileDeleteService - HdfsTempFileDeleteService_线程扫描..






2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - start parseTableIdInfo,tableId:TableId(catalogName=null, schemaName=test01, tableName=b_foitem_copy, metaType=null, tableALias=null),datasourceInfo:mrs_hive_spark_test,currentDB:,currentSchema:default, tempTableMapInfo:{}
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - tableName:b_foitem_copy
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - current schema:test01
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - start get mrs_hive_spark_test 's datasourceName,db:test01
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - data connections size:4
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - current db's datasource name:mrs_hive_spark_test,env:2
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - current datasource name:(mrs_hive_spark_test,2),currentEnv:2
2026-01-13 09:36:47.087 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - start checkIsOwner,catalog:mrs_hive_spark_test360,database:null,schema:test01,tableName:b_foitem_copy
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - ownerId:1
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - performUserId:1 is owner
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.SecurityUtilService - -----read
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - accessLogs size:0
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.security.AbstractPrivilegesService - getParseInfoByMqDtoOver:[]
2026-01-13 09:36:47.107 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - 1 spark: taskDispatchMqDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010888723605905410, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 09:36:46 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/1768268206981.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: 执行executeTask线程名称:SPARK_SQL-TaskExecutor-2010888723605905410
2026-01-13 09:36:47.107 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - 2 spark: currentDispatchDto: TaskDispatchMqDto(resourceGroupId=1, taskType=SPARK_SQL, env=temp, convertedEnv=dev, tenantId=1001, projectId=1968500511214804993, mode=NORM, taskInstanceId=2010888723605905410, parentInstanceId=null, clusterName=iam_spark_dev, clusterType=HADOOP, storageClusterName=null, storageClusterId=1992956917615718402, taskName=test_saprk, taskConfig=[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}], taskId=1998681423015882754, structureType=null, businessTime=Mon Jan 12 00:00:00 CST 2026, scheduleTime=Tue Jan 13 09:36:46 CST 2026, timeoutDuration=60, taskContent={"sqlContent":"SELECT * from test01.b_foitem_copy;","submitParam":[{"key":"isOpenAQE","value":"false"},{"key":"spark.driver.cores","value":"1"},{"key":"spark.driver.memory","value":"1g"},{"key":"spark.executor.cores","value":"1"},{"key":"spark.executor.instances","value":"1"},{"key":"spark.executor.memory","value":"2g"}]}, enforce=null, globalVariables={}, localVariables={}, componentInputParams=[], componentOutputParams=[], contextInputParameters=[], contextOutputParameters=null, operateType=START, deployMode=null, isSessionClusterTask=false, isSessionTask=false, sessionClusterName=null, sendTime=null, flinkProvidedDirs=null, flinkDependencyFileDirs=null, clusterVersion=null, hadoopUserName=null, onS3=false, workPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410, logPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/1768268206981.log, logResultPath=/opt/app/instanceLogs/20260113/instanceLogs/1968500511214804993/temp/2010888723605905410/0123456789.log, applicationId=null, jobId=null, isKerberos=null, runMode=1, performUserId=1, performUser=SuperAdmin, performType=2, chargeUserId=112, chargeUser=liuAdmin2, kubernetesNamespace=null, kubernetesConfig=null, s3Bucket=null, s3Endpoint=null, s3AccessKey=null, s3SecretKey=null, pipeline=null, jdbcLoop=null, hiveConfHdfsDir=null, costTime=null, taskRunUserId=1, useWholeDB=false)
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover获取到的appId为空
2026-01-13 09:36:47.107 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark: spark failover结果:false
2026-01-13 09:36:47.110 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager - 集群:1001~~2~~iam_spark_dev 加载失败: java.lang.IllegalArgumentException: kerberosConf can not null, engine code: MRS_360_SPARK_TEST_DEV
        at org.springframework.util.Assert.notNull(Assert.java:201)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.downloadClusterConfig(ClusterEngineManager.java:596)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.loadEngine(ClusterEngineManager.java:223)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.lambda$afterPropertiesSet$0(ClusterEngineManager.java:141)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

2026-01-13 09:36:47.110 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - Task execute failed 
com.datacyber.cyberdata.worker.exception.WorkerException: 集群:1001~~2~~iam_spark_dev 加载失败: java.lang.IllegalArgumentException: kerberosConf can not null, engine code: MRS_360_SPARK_TEST_DEV
        at org.springframework.util.Assert.notNull(Assert.java:201)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.downloadClusterConfig(ClusterEngineManager.java:596)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.loadEngine(ClusterEngineManager.java:223)
        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.lambda$afterPropertiesSet$0(ClusterEngineManager.java:141)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
        at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)

        at com.datacyber.cyberdata.worker.support.cluster.ClusterEngineManager.getHadoopConfigDir(ClusterEngineManager.java:463)
        at com.datacyber.cyberdata.worker.service.SparkSqlExecutorService.buildYarnCommandOfNew(SparkSqlExecutorService.java:115)
        at com.datacyber.cyberdata.worker.service.SparkSqlExecutorService.buildCommand(SparkSqlExecutorService.java:94)
        at com.datacyber.cyberdata.worker.service.SparkExecutorService.executeTask(SparkExecutorService.java:178)
        at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor.runTask(AbstractTaskExecutor.java:310)
        at com.datacyber.cyberdata.worker.service.AbstractTaskExecutor$$FastClassBySpringCGLIB$$4b20b1cc.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithSpan(SpannedMethodInvocation.java:50)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.invokeWithContinuation(SpannedMethodInvocation.java:42)
        at datadog.trace.instrumentation.springscheduling.SpannedMethodInvocation.proceed(SpannedMethodInvocation.java:36)
        at org.springframework.aop.interceptor.AsyncExecutionInterceptor.lambda$invoke$0(AsyncExecutionInterceptor.java:115)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2026-01-13 09:36:47.111 ERROR [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.SparkExecutorService - spark application id is empty
2026-01-13 09:36:47.111 INFO  [cyber-worker] [SPARK_SQL-TaskExecutor-2010888723605905410] com.datacyber.cyberdata.worker.service.AbstractTaskExecutor - handleInstanceState: localVariables:{}












日志获取中
2026-01-12 09:59:11 INFO  Worker: 10.131.8.25:7800 receives the metadata collection task
2026-01-12 09:59:11 INFO  Metadata collection parameters information:
2026-01-12 09:59:11 INFO  【taskId】:1911, 【recordId】:63391
2026-01-12 09:59:11 INFO  【datasource type】:HIVE
2026-01-12 09:59:11 INFO  【datasource id】:2009875419122475009
2026-01-12 09:59:11 INFO  【datasource connection id】:2009875419193778177
2026-01-12 09:59:11 INFO  【datasource jdbc url】:jdbc:hive2://mrs-adp-q02-node-master2femp.mrs-h69n.com:22550/test01_dev
2026-01-12 09:59:11 INFO  【database name】:test01_dev
2026-01-12 09:59:58 ERROR Hive getMetaData occurred exception.
java.lang.RuntimeException: Authentication: simple 操作失败: null
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.runSecured(Hive3Plugin.java:103)
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.getConnection(Hive3Plugin.java:198)
	at com.datacyber.cyberdata.worker.service.accessor.impl.HiveAccessService.getMetaData(HiveAccessService.java:130)
	at com.datacyber.cyberdata.worker.service.metadata.MetadataCollectorService.lambda$collect$0(MetadataCollectorService.java:253)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.reflect.UndeclaredThrowableException: null
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1894)
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.runSecured(Hive3Plugin.java:99)
	... 8 common frames omitted
Caused by: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://mrs-adp-q02-node-master2femp.mrs-h69n.com:22550/test01_dev: Peer indicated failure: PLAIN auth failed: org.apache.hive.service.auth.basic.BasicAuthenticationException: user[anonymous] basic authentication failed.
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:256)
	at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:677)
	at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
	at com.datacyber.cybermeta.jdbc.plugins.jdbc.DriverConnectionFactoryNoPool.openConnection(DriverConnectionFactoryNoPool.java:50)
	at com.datacyber.cybermeta.jdbc.plugins.Hive3Plugin$HiveJdbcDialect.getHiveConnection(Hive3Plugin.java:203)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	... 9 common frames omitted
Caused by: org.apache.thrift.transport.TTransportException: Peer indicated failure: PLAIN auth failed: org.apache.hive.service.auth.basic.BasicAuthenticationException: user[anonymous] basic authentication failed.
	at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:199)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:307)
	at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
	at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:343)
	at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:228)
	... 17 common frames omitted
2026-01-12 09:59:58 INFO  Thread【MetaDataCollector_sub_thread_63391_0】Complete the current batch collection, table of count: 0
2026-01-12 09:59:58 INFO  Start collecting object information.
2026-01-12 09:59:58 INFO  Complete collect object information.
2026-01-12 09:59:58 INFO  The metadata is retrieved and ready to be written to persistent storage.
2026-01-12 09:59:58 INFO  A total of 0 tables are found in this collection task, which are written to persistent storage in 0 batches.
2026-01-12 09:59:58 INFO  Writes to persistent storage have been completed
2026-01-12 09:59:58 INFO  The collection task has been completed, A total of 0 tables have been collected.
2026-01-12 09:59:58 INFO  Clean up the collection context information
2026-01-12 09:59:58 INFO  END-EOF

帮助




25/11/27 17:34:04 INFO RetryInvocationHandler: Call From cyber-worker-test-8574cd4775-m8xkp/10.131.15.83 to mrs-adp-q02-node-master2femp.mrs-h69n.com:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over 18 after 19 failover attempts. Trying to failover after sleeping for 42745ms.
25/11/27 17:34:47 INFO ConfiguredRMFailoverProxyProvider: Failing over to 19
25/11/27 17:34:47 INFO RetryInvocationHandler: DestHost:destPort mrs-adp-q02-node-master3qkoz.mrs-h69n.com:8032 , LocalHost:localPort cyber-worker-test-8574cd4775-m8xkp/10.131.15.83:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Server has invalid Kerberos principal: mapred/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@B2D05EE0_E921_40A8_A6FD_DCB7693B106B.COM, expecting: mapred/hadoop.950c233d_ab6e_4ad9_ab28_f2766f074340.com@950C233D_AB6E_4AD9_AB28_F2766F074340.COM, while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over 19 after 20 failover attempts. Trying to failover after sleeping for 33880ms.



2025-11-17 14:09:38.002 ERROR [cyber-platform] [http-nio-7600-exec-113] [] com.datacyber.cyberdata.service.core.exception.CoreExceptionHandler - Exception: request: /standard/field/export/template raised List validation with explicit values must specify at least one value
java.lang.IllegalArgumentException: List validation with explicit values must specify at least one value
        at org.apache.poi.xssf.usermodel.XSSFDataValidationConstraint.<init>(XSSFDataValidationConstraint.java:50)
        at org.apache.poi.xssf.usermodel.XSSFDataValidationHelper.createExplicitListConstraint(XSSFDataValidationHelper.java:66)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.addDropdownToColumn(DdsFieldServiceImpl.java:1881)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.access$000(DdsFieldServiceImpl.java:127)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl$2.afterSheetCreate(DdsFieldServiceImpl.java:1799)
        at com.alibaba.excel.write.handler.SheetWriteHandler.afterSheetCreate(SheetWriteHandler.java:37)
        at com.alibaba.excel.write.handler.chain.SheetHandlerExecutionChain.afterSheetCreate(SheetHandlerExecutionChain.java:40)
        at com.alibaba.excel.util.WriteHandlerUtils.afterSheetCreate(WriteHandlerUtils.java:98)
        at com.alibaba.excel.util.WriteHandlerUtils.afterSheetCreate(WriteHandlerUtils.java:92)
        at com.alibaba.excel.context.WriteContextImpl.initSheet(WriteContextImpl.java:206)
        at com.alibaba.excel.context.WriteContextImpl.currentSheet(WriteContextImpl.java:135)
        at com.alibaba.excel.write.ExcelBuilderImpl.addContent(ExcelBuilderImpl.java:54)
        at com.alibaba.excel.ExcelWriter.write(ExcelWriter.java:73)
        at com.alibaba.excel.ExcelWriter.write(ExcelWriter.java:50)
        at com.alibaba.excel.write.builder.ExcelWriterSheetBuilder.doWrite(ExcelWriterSheetBuilder.java:62)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.downloadWithDynamicHead(DdsFieldServiceImpl.java:1689)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl.exportTemplate(DdsFieldServiceImpl.java:1565)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl$$FastClassBySpringCGLIB$$a47b2c4.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:707)
        at com.datacyber.cyberdata.service.dds.service.impl.DdsFieldServiceImpl$$EnhancerBySpringCGLIB$$69b87128.exportTemplate(<generated>)
        at com.datacyber.cyberdata.controller.dds.DdsFieldController.exportTemplate(DdsFieldController.java:230)
        at com.datacyber.cyberdata.controller.dds.DdsFieldController$$FastClassBySpringCGLIB$$870c4379.invoke(<generated>)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:218)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:792)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:89)
        at com.datacyber.cyberdata.common.aspect.AESCryptoOperationAop.aesOperation(AESCryptoOperationAop.java:49)
        at jdk.internal.reflect.GeneratedMethodAccessor184.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:634)
        at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:624)
        at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:72)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186)
        at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.proceed(CglibAopProxy.java:762)
        at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:707)
        at com.datacyber.cyberdata.controller.dds.DdsFieldController$$EnhancerBySpringCGLIB$$8e2984ab.exportTemplate(<generated>)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
        at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
        at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
        at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
        at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
        at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
        at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
        at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:645)
        at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:750)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at com.datacyber.cyberdata.service.filter.LicenseRoleAuthorizationFilter.doFilterInternal(LicenseRoleAuthorizationFilter.java:79)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:111)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:337)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:115)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:81)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:122)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:116)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:126)
        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:81)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:109)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:149)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at com.datacyber.cyber.user.sdk.filter.JwtAuthenticationTokenFilter.doFilterInternal(JwtAuthenticationTokenFilter.java:83)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:111)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:103)
        at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:89)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.web.filter.CorsFilter.doFilterInternal(CorsFilter.java:91)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.header.HeaderWriterFilter.doHeadersAfter(HeaderWriterFilter.java:90)
        at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:75)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:112)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:82)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:55)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.session.DisableEncodeUrlFilter.doFilterInternal(DisableEncodeUrlFilter.java:42)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:346)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:221)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:186)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at datadog.trace.instrumentation.springweb.HandlerMappingResourceNameFilter.doFilterInternal(HandlerMappingResourceNameFilter.java:50)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at com.datacyber.cyber.user.sdk.filter.ExceptionFilter.doFilterInternal(ExceptionFilter.java:22)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.springframework.web.filter.ServletRequestPathFilter.doFilter(ServletRequestPathFilter.java:56)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
        at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
        at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:765)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
        at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
        at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
        at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
        at org.apache.tomcat.util.net.NioEndpoint$SocketProcesso



[2025-11-05 18:37:45.132] [traceId=54da77d1663e065339dd72a429c8ed56 spanId=752087985961214363] [service=cyber-usercenter-test ] [c.d.c.u.s.s.impl.SysDataServiceImpl] [http-nio-8080-exec-6] [ERROR] Failed to get token from /sdkAuth/getToken: 400 : "<!doctype html><html lang="en"><head><title>HTTP Status 400 – Bad Request</title><style type="text/css">body {font-family:Tahoma,Arial,sans-serif;} h1, h2, h3, b {color:white;background-color:#525D76;} h1 {font-size:22px;} h2 {font-size:16px;} h3 {font-size:14px;} p {font-size:12px;} a {color:black;} .line {height:1px;background-color:#525D76;border:none;}</style></head><body><h1>HTTP Status 400 – Bad Request</h1></body></html>"







https://cnp.anta.com/kpanda/clusters/

http://10.131.128.80:7799/api/tenant/mrs/user/export?managerUrl=https://10.131.194.230:9022/mrsmanager/&password=km72QG3Npy!1&username=u_adp_manager



https://authtest.anta.com/logout?service=https://authtest.anta.com/oauth2.0/authorize?response_type=code&client_id=100264&redirect_uri=https://adp-test.anta.com/index


[2025-09-23 10:32:42.316] [traceId=cca5ec05f94c96e4feeb01a510f4be7d spanId=748098214889791199] [service=cyber-worker-test ] [c.d.c.w.a.s.DatasourceQueryService] [http-nio-7800-exec-5] [INFO ] datasourceQuery enter: {"env":"dev","info":{"baseInputParamModelsJson":"[]","database":"poc_test","datasourceConnectionJson":"{\"catalogName\":\"doris_test\",\"collectSwitch\":1,\"connectivityInfo\":\"{\\\"connectivityStatus\\\":true,\\\"errorMessage\\\":\\\"\\\",\\\"resourceGroupId\\\":1,\\\"testTime\\\":\\\"2025-09-22T09:23:14.652\\\"}\",\"connectivityStatus\":1,\"createEngineSwitch\":1,\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"datasourceId\":1968879021349781506,\"detail\":\"{\\\"feAddress\\\":[\\\"18030\\\"],\\\"password\\\":\\\"/+PcZuJ8EnhfeBU1kxMNow==\\\",\\\"starRocksFEAddress\\\":\\\"18030\\\",\\\"url\\\":\\\"jdbc:mysql://192.168.99.9:19030/\\\",\\\"username\\\":\\\"poc_test\\\"}\",\"env\":2,\"host\":\"192.168.99.9\",\"id\":1968879021840515074,\"port\":\"19030\",\"state\":1,\"testTime\":\"2025-09-22T09:23:15\",\"updateTime\":\"2025-09-19T11:25:13\"}","datasourceJson":"{\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"customProperties\":\"[]\",\"description\":\"\",\"id\":1968879021349781506,\"isFreeze\":0,\"mode\":2,\"name\":\"doris_test\",\"state\":1,\"tenantId\":1001,\"type\":\"DORIS\",\"updateTime\":\"2025-09-19T11:25:13\",\"updateUser\":1,\"version\":0}","isFromWizardl":true,"mode":2,"outputParamModelsJson":"[{\"fieldName\":\"id\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"yearNum\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"monthNum\",\"fieldType\":\"STRING\",\"isPaging\":false,\"sampleVale\":\"\"}]","page":true,"sourceSupportEnum":"DORIS","sql":"SELECT * FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2`  ) t LIMIT 10 OFFSET 0","test":true,"totalSql":"SELECT count(*) AS total FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2` ) t","userId":48},"methodType":"API_SERVER_SQL","requestId":1758594762308,"resourceGroupId":1,"taskType":"DATASOURCE_QUERY"}









[2025-09-23 09:32:07.684] [traceId=ec22eafa680e1d9be7ac9d21e3c973d3 spanId=6230099790631525190] [service=cyber-worker-test ] [c.d.c.w.a.s.DatasourceQueryService] [http-nio-7800-exec-7] [INFO ] datasourceQuery enter: {"env":"prod","info":{"baseInputParamModelsJson":"[]","datasourceConnectionJson":"{\"catalogName\":\"doris_test\",\"collectSwitch\":1,\"connectivityInfo\":\"{\\\"connectivityStatus\\\":true,\\\"errorMessage\\\":\\\"\\\",\\\"resourceGroupId\\\":1,\\\"testTime\\\":\\\"2025-09-22T09:23:08.297\\\"}\",\"connectivityStatus\":1,\"createEngineSwitch\":1,\"createTime\":\"2025-09-19T11:25:14\",\"createUser\":1,\"datasourceId\":1968879021349781506,\"detail\":\"{\\\"feAddress\\\":[\\\"18030\\\"],\\\"password\\\":\\\"/+PcZuJ8EnhfeBU1kxMNow==\\\",\\\"starRocksFEAddress\\\":\\\"18030\\\",\\\"url\\\":\\\"jdbc:mysql://192.168.99.9:19030/\\\",\\\"username\\\":\\\"poc_test\\\"}\",\"env\":5,\"host\":\"192.168.99.9\",\"id\":1968879022507409409,\"port\":\"19030\",\"state\":1,\"testTime\":\"2025-09-22T09:23:08\",\"updateTime\":\"2025-09-19T11:25:13\"}","datasourceJson":"{\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"customProperties\":\"[]\",\"description\":\"\",\"id\":1968879021349781506,\"isFreeze\":0,\"mode\":2,\"name\":\"doris_test\",\"state\":1,\"tenantId\":1001,\"type\":\"DORIS\",\"updateTime\":\"2025-09-19T11:25:13\",\"updateUser\":1,\"version\":0}","isFromWizardl":true,"mode":2,"outputParamModelsJson":"[{\"fieldName\":\"id\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"yearNum\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"monthNum\",\"fieldType\":\"STRING\",\"isPaging\":false,\"sampleVale\":\"\"}]","page":true,"sourceSupportEnum":"DORIS","sql":"SELECT * FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2`  ) t LIMIT 10 OFFSET 0","test":true,"totalSql":"SELECT count(*) AS total FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2` ) t","userId":48},"methodType":"API_SERVER_SQL","requestId":1758591127677,"resourceGroupId":1,"taskType":"DATASOURCE_QUERY"}



[2025-09-23 09:32:07.695] [traceId=ec22eafa680e1d9be7ac9d21e3c973d3 spanId=6230099790631525190] [service=cyber-worker-test ] [c.d.c.w.a.s.DatasourceQueryService] [http-nio-7800-exec-7] [ERROR] datasourceQuery failed: param:{"connInfo":{"dataSourceType":"DORIS","datasourceConnectionId":1968879022507409409,"jdbcUrl":"jdbc:mysql://192.168.99.9:19030/","password":"poctest.321","username":"poc_test","version":0},"env":"prod","info":{"baseInputParamModelsJson":"[]","datasourceConnectionJson":"{\"catalogName\":\"doris_test\",\"collectSwitch\":1,\"connectivityInfo\":\"{\\\"connectivityStatus\\\":true,\\\"errorMessage\\\":\\\"\\\",\\\"resourceGroupId\\\":1,\\\"testTime\\\":\\\"2025-09-22T09:23:08.297\\\"}\",\"connectivityStatus\":1,\"createEngineSwitch\":1,\"createTime\":\"2025-09-19T11:25:14\",\"createUser\":1,\"datasourceId\":1968879021349781506,\"detail\":\"{\\\"feAddress\\\":[\\\"18030\\\"],\\\"password\\\":\\\"/+PcZuJ8EnhfeBU1kxMNow==\\\",\\\"starRocksFEAddress\\\":\\\"18030\\\",\\\"url\\\":\\\"jdbc:mysql://192.168.99.9:19030/\\\",\\\"username\\\":\\\"poc_test\\\"}\",\"env\":5,\"host\":\"192.168.99.9\",\"id\":1968879022507409409,\"port\":\"19030\",\"state\":1,\"testTime\":\"2025-09-22T09:23:08\",\"updateTime\":\"2025-09-19T11:25:13\"}","datasourceJson":"{\"createTime\":\"2025-09-19T11:25:13\",\"createUser\":1,\"customProperties\":\"[]\",\"description\":\"\",\"id\":1968879021349781506,\"isFreeze\":0,\"mode\":2,\"name\":\"doris_test\",\"state\":1,\"tenantId\":1001,\"type\":\"DORIS\",\"updateTime\":\"2025-09-19T11:25:13\",\"updateUser\":1,\"version\":0}","isFromWizardl":true,"mode":2,"outputParamModelsJson":"[{\"fieldName\":\"id\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"yearNum\",\"fieldType\":\"INT\",\"isPaging\":false,\"sampleVale\":\"\"},{\"fieldName\":\"monthNum\",\"fieldType\":\"STRING\",\"isPaging\":false,\"sampleVale\":\"\"}]","page":true,"sourceSupportEnum":"DORIS","sql":"SELECT * FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2`  ) t LIMIT 10 OFFSET 0","test":true,"totalSql":"SELECT count(*) AS total FROM ( SELECT `id`, `yearNum`, `monthNum` FROM `dim_date2` ) t","userId":48},"methodType":"API_SERVER_SQL","requestId":1758591127677,"resourceGroupId":1,"taskType":"DATASOURCE_QUERY"}, ex:

java.sql.SQLException: errCode = 2, detailMessage = No database selected
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:130)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:916)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:972)
	at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeQuery(ProxyPreparedStatement.java:52)
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeQuery(HikariProxyPreparedStatement.java)
	at com.datacyber.cyberdata.worker.api.service.DatasourceQueryService.getApiServerSqlQueryResult(DatasourceQueryService.java:276)
	at com.datacyber.cyberdata.worker.api.service.DatasourceQueryService.datasourceQuery(DatasourceQueryService.java:236)
	at com.datacyber.cyberdata.worker.api.controller.DatasourceController.datasourceQuery(DatasourceController.java:39)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:150)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:117)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:895)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1072)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:965)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:555)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:623)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:209)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at datadog.trace.instrumentation.springweb.HandlerMappingResourceNameFilter.doFilterInternal(HandlerMappingResourceNameFilter.java:50)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:96)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:117)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.springframework.web.filter.ServletRequestPathFilter.doFilter(ServletRequestPathFilter.java:56)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:354)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:267)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:178)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:153)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:168)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:481)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:130)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)
	at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:765)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:342)
	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:390)
	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)
	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:928)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1794)
	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1191)
	at org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.base/java.lang.Thread.run(Thread.java:833)
